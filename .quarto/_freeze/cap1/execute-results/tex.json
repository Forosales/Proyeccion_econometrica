{
  "hash": "05b081066611541399aa2bbe9c8a2749",
  "result": {
    "engine": "knitr",
    "markdown": "# Variable dependiente binaria\n\n## El modelo de probabilidad lineal (MPL)\n\n¿Qué ocurre cuando se desea usar la regresión múltiple para explicar eventos cualitativos?\n\nEl caso más sencillo es un evento del tipo binario, es decir, que $y$ toma valores de cero y uno. Por ejemplo, $y$ puede indicar si una persona trabaja o no trabaja, esta empleado o desempleado o, si una empresa es grande o pequeña. En cualquier caso se puede hacer que $y=1$ denota uno de los resultados o $y=0$ denota el otro resultado. También podría pensarse como éxito y fracaso. La demostración parte de la función de regresión poblacional.\n\n$$\nY=\\beta_0+\\beta_1X_1+\\beta_2X_2+...+\\beta_kX_k+u \\hspace{0.5cm}[1]\n$$\n\nComo $y$ solo puede toma dos valores, los $\\beta_j$ **no** pueden interpretarse como una cambio en $y$ para un aumento de $x_j$ **ceteris paribus**. Recordar que en este caso $y$ cambia de cero a uno, o no cambia. Partiendo del supuesto de **media condicional cero** $E(u|X_1, X_2,...,X_k)=0$, tenemos:\n\n$$\nE(y|\\mathbf{X})=\\beta_0+\\beta_1X_1+\\beta_2X_2+...+\\beta_kX_k+u \\hspace{1cm}[2]\n$$\n\nEl punto clave es que $y$ es una variable binaria que toma valores de cero y uno, entonces tenemos que $P(y=1|\\mathbf{X})=E(y|\\mathbf{X})$: la probabilidad de **\"éxito\"**. Por lo tanto, tenemos\n\n$$\nP(y=1|\\mathbf{X})=\\beta_0+\\beta_1X_1+\\beta_2X_2+...+\\beta_kX_k+u \\hspace{0.5cm} [3]\n$$\n\nLa ecuación \\[3\\] dice que, la probabilidad de éxito, es decir $p(x)=P(y=1|\\mathbf{X})$, es una función lineal de las variables $x_j$, también se le conoce como la **probabilidad de respuesta**. Dado que, las probabilidades deben sumar uno, $P(y=0|\\mathbf{X})=1-P(y=1|\\mathbf{X})$, es también una función lineal de las $x_j$\n\nPor lo tanto, a un modelo de regresión lineal múltiple en el que la variable dependiente es una variable binaria se le conoce como: **El modelo de probabilidad lineal (MPL)**, porque la probabilidad de respuesta es lineal a los parámetros $\\beta_j$. En el MPL, los $\\beta_j$ miden la variación de la probabilidad de éxito de variar $x_j$ **ceteris paribus**:\n\n$$\n\\Delta P(y=1|X)=\\beta_j \\Delta x_j \\hspace{0.5cm} [4]\n$$\n\n### Ejemplo en la clase\n\nLos determinantes del desempleo\n\n$$\nP(de=1|educ)=\\beta_0+\\beta_1educ+u \\hspace{0.5cm} ej1\n$$\n\n$$\nP(de=1|educ)=0.23-1.04educ+u \\hspace{0.5cm} ej2\n$$\n\nDonde:\n\n-   de: Desempleo, 1 si estas desempleado. 0 otro caso\n\n-   educ: años de educación\n\nEl modelo de regresión lineal múltiple permite estimar el efecto de diversas variables explicativas sobre un evento cualitativo. Entonces la mecánica de los **MCO** es la misma de siempre:\n\n$$\n\\widehat{y}=\\widehat{\\beta}_0+\\widehat{\\beta}_1x_1+\\widehat{\\beta}_2x_2+...+\\widehat{\\beta}_kx_k \\hspace{0.5cm} [5]\n$$ Donde:\n\n-   $\\widehat{y}$: es la probabilidad de éxito predicha\n\n-   $\\widehat{\\beta}_0$: es la probabilidad de éxito cuando cada una de las $x_j=0$\n\n-   $\\widehat{\\beta}_1$: mide la variación de la probabilidad de éxito predicha cuando $x_1$ varia en una unidad, mientras las demás permanecen constantes.\n\n-   $\\widehat{\\beta}_j$: mide la variación de la probabilidad de éxito predicha cuando $x_j$ varia en una unidad, mientras las demás permanecen constantes.\n\n**¡¡Concepto clave: para interpretar correctamente un MPL, debe saberse qué es lo que constituye el éxito!!**\n\n**Recomendación**: la variable dependiente debe describir el nombre del evento cuando $y=1$\n\nPor ejemplo, si estudiamos los determinantes del desempleo, la variable $y$ debe llamarse desempleo\n\n## Ejemplo 1: Determinantes de la denegación en solicitudes de hipoteca en el mercado inmobiliario [@stock2012]\n\n-   $denegar=1$ le negaron la hipoteca y $0$ otro caso\n\nVariable explicativa:\n\n-   $\\frac{P}{I}$: Ratio Pagos-ingresos\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npacman::p_load(AER, \n               stargazer,\n               sandwich)\n\ndata(\"HMDA\")\nstr(HMDA)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n'data.frame':\t2380 obs. of  14 variables:\n $ deny     : Factor w/ 2 levels \"no\",\"yes\": 1 1 1 1 1 1 1 1 2 1 ...\n $ pirat    : num  0.221 0.265 0.372 0.32 0.36 ...\n $ hirat    : num  0.221 0.265 0.248 0.25 0.35 ...\n $ lvrat    : num  0.8 0.922 0.92 0.86 0.6 ...\n $ chist    : Factor w/ 6 levels \"1\",\"2\",\"3\",\"4\",..: 5 2 1 1 1 1 1 2 2 2 ...\n $ mhist    : Factor w/ 4 levels \"1\",\"2\",\"3\",\"4\": 2 2 2 2 1 1 2 2 2 1 ...\n $ phist    : Factor w/ 2 levels \"no\",\"yes\": 1 1 1 1 1 1 1 1 1 1 ...\n $ unemp    : num  3.9 3.2 3.2 4.3 3.2 ...\n $ selfemp  : Factor w/ 2 levels \"no\",\"yes\": 1 1 1 1 1 1 1 1 1 1 ...\n $ insurance: Factor w/ 2 levels \"no\",\"yes\": 1 1 1 1 1 1 1 1 2 1 ...\n $ condomin : Factor w/ 2 levels \"no\",\"yes\": 1 1 1 1 1 1 2 1 1 1 ...\n $ afam     : Factor w/ 2 levels \"no\",\"yes\": 1 1 1 1 1 1 1 1 1 1 ...\n $ single   : Factor w/ 2 levels \"no\",\"yes\": 1 2 1 1 1 1 2 1 1 2 ...\n $ hschool  : Factor w/ 2 levels \"no\",\"yes\": 2 2 2 2 2 2 2 2 2 2 ...\n```\n\n\n:::\n:::\n\n\n\n\nEl objetivo es estimar la siguiente ecuación\n\n$$\nP(Y=1|x_1)=\\beta_0+\\beta_1x_1+u \\hspace{0.5cm} [6]\n$$\n\nahora usando nuestra ejemplo:\n\n$$\nP(deny=1|P/I)=\\beta_0+\\beta_1P/I+u\\hspace{0.5cm} [7]\n$$\n\nMiremos las variables que ingresan. Primero hacemos binaria a la variable *deny*\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nHMDA$deny <- ifelse(HMDA$deny==\"yes\",\n                    1,\n                    0)\n# Estimar el modelo de probabilidad lineal\n\ndenymod1 <- lm(deny~pirat, \n               HMDA)\nstargazer(denymod1, \n          type = \"text\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n===============================================\n                        Dependent variable:    \n                    ---------------------------\n                               deny            \n-----------------------------------------------\npirat                        0.604***          \n                              (0.061)          \n                                               \nConstant                     -0.080***         \n                              (0.021)          \n                                               \n-----------------------------------------------\nObservations                   2,380           \nR2                             0.040           \nAdjusted R2                    0.039           \nResidual Std. Error      0.318 (df = 2378)     \nF Statistic          98.406*** (df = 1; 2378)  \n===============================================\nNote:               *p<0.1; **p<0.05; ***p<0.01\n```\n\n\n:::\n:::\n\n\n\n\nSeria $\\widehat{\\beta_1}=0.604\\times0.01\\approx 0.06$\n\nUna vez que tengo el MPL, puedo graficar\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(x = HMDA$pirat,\n     y = HMDA$deny,\n     main = \"Gráfico de dispersión de las denegaciones de hipoteca y el ratio pagos-ingresos\", \n     xlab = \"ratio P/I\", \n     ylab = \"Denegar\", \n     pch = 20, \n     ylim = c(-0.4,1.4), \n     cex.main = 0.8)\n\n# Añadir las lineas horizontales\n\nabline(h=1, lty = 2, col = \"darkred\")\nabline(h=0, lty = 2, col = \"darkred\")\n\ntext(2.5, 0.9, \n     cex = 0.8, \n     \"Hipoteca denegada\")\n\ntext(2.5, -0.1, \n     cex = 0.8, \n     \"Hipoteca concedida\")\n\n# Añadiendo la linea del MPL\n\nabline(denymod1, \n       lwd =0.8,\n       col = \"steelblue\")\n\ntext(1.25, 0.4, \n     cex = 0.8, \n     \"Modelo de probabilidad lineal\")\n```\n\n::: {.cell-output-display}\n![](cap1_files/figure-pdf/grafico-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\nPresentación de la regresión\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(stargazer)\nlibrary(sandwich)\neshr <- list(sqrt(diag(vcovHC(denymod1, \n                              type = \"HC1\"))))\n\nstargazer(denymod1,denymod1, \n          type = \"text\", \n          se = eshr)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n============================================================\n                                    Dependent variable:     \n                                ----------------------------\n                                            deny            \n                                     (1)            (2)     \n------------------------------------------------------------\npirat                              0.604***      0.604***   \n                                   (0.098)        (0.061)   \n                                                            \nConstant                           -0.080**      -0.080***  \n                                   (0.032)        (0.021)   \n                                                            \n------------------------------------------------------------\nObservations                        2,380          2,380    \nR2                                  0.040          0.040    \nAdjusted R2                         0.039          0.039    \nResidual Std. Error (df = 2378)     0.318          0.318    \nF Statistic (df = 1; 2378)        98.406***      98.406***  \n============================================================\nNote:                            *p<0.1; **p<0.05; ***p<0.01\n```\n\n\n:::\n:::\n\n\n\n\nComo los modelos de regresión lineal simple poseen el problema de sesgo de variable omitida, y de debido a que el gráfico muestra comportamientos que no son solo explicados por la variable independiente (*pirat*), se añade otra variable que puede ayudar a explicar el fenómeno. La variable es respecto a la conseción de hipoteca a las personas negras (*afam*)\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncolnames(HMDA)[13] <- \"negra\"\n\ndenymod2 <- lm(deny ~ \n                 pirat + \n                 negra, \n               data = HMDA)\n\neshr <- list(sqrt(diag(vcovHC(denymod1, \n                              type = \"HC1\"))),\n             sqrt(diag(vcovHC(denymod2, \n                              type = \"HC1\"))))\n\n\nstargazer(denymod1, \n          denymod2,\n          type = \"text\",\n          se = eshr,\n          df = F, \n          report = \"vcts*\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n================================================\n                        Dependent variable:     \n                    ----------------------------\n                                deny            \n                         (1)            (2)     \n------------------------------------------------\npirat                   0.604          0.597    \n                      t = 6.128      t = 6.247  \n                      (0.098)***    (0.096)***  \n                                                \nnegrayes                               0.047    \n                                     t = 3.387  \n                                    (0.014)***  \n                                                \nConstant                -0.080        -0.096    \n                      t = -2.500    t = -3.105  \n                      (0.032)**     (0.031)***  \n                                                \n------------------------------------------------\nObservations            2,380          2,380    \nR2                      0.040          0.045    \nAdjusted R2             0.039          0.044    \nResidual Std. Error     0.318          0.318    \nF Statistic           98.406***      55.614***  \n================================================\nNote:                *p<0.1; **p<0.05; ***p<0.01\n```\n\n\n:::\n:::\n\n\n\n\nMiremos los valores ajustados\n\n$\\widehat{y}_i$\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# modelo con un solo regresor\nsummary(denymod1$fitted.values)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-0.07991  0.08908  0.11926  0.11975  0.14340  1.73070 \n```\n\n\n:::\n\n```{.r .cell-code}\n# modelo con dos regresores\n\nsummary(denymod2$fitted.values)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-0.09614  0.08800  0.11874  0.11975  0.14859  1.69459 \n```\n\n\n:::\n:::\n\n\n\n\n### Porcentaje predicho correctamente\n\nMirada a los valores ajustados\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndenymod2$fitted.values\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n            1             2             3             4             5 \n 0.0357740938  0.1088880900  0.1259074499  0.0948681447  0.1187445298 \n            6             7             8             9            10 \n 0.0471153745  0.1596254083  0.0709917596  0.0888990484  0.0581507716 \n           11            12            13            14            15 \n 0.1178417344  0.1357490232  0.0709917596  0.1187445298  0.0113007969 \n           16            17            18            19            20 \n 0.1187445298  0.0411462782  0.0948681447  0.0888990484  0.1715636008 \n           21            22            23            24            25 \n 0.1247136260  0.1366518186  0.1417181195  0.1366518186  0.1476872157 \n           26            27            28            29            30 \n 0.1178417344  0.1715636008  0.1297799269  0.0888990484  0.1059035418 \n           31            32            33            34            35 \n 0.1655945046  0.1247136260  0.1596254083  0.1715636008  0.1068063372 \n           36            37            38            39            40 \n 0.1306827223  0.1664973000  0.1127754335  0.1306827223  0.1715636008 \n           41            42            43            44            45 \n 0.1247136260  0.1485900111  0.1127754335  0.1485900111  0.1536563120 \n           46            47            48            49            50 \n 0.0769608559  0.0650226633  0.1715636008  0.1068063372  0.1655945046 \n           51            52            53            54            55 \n 0.1536563120  0.0650226633  0.0462125791  0.1835017934  0.1187445298 \n           56            57            58            59            60 \n 0.0912866846  0.1068063372  0.1488810396  0.1476872157  0.1888739891 \n           61            62            63            64            65 \n 0.1080001610  0.1751450495  0.0954650452  0.0703948477  0.0930774204 \n           66            67            68            69            70 \n 0.0739763077  0.1217290779  0.1050156129  0.1596254083  0.1608192321 \n           71            72            73            74            75 \n 0.1127754335  0.1091939848  0.1178417344  0.1294888985  0.0578597432 \n           76            77            78            79            80 \n 0.0339833581  0.0411462782  0.0739763077  0.0620381152  0.1655945046 \n           81            82            83            84            85 \n 0.0662164871  0.1655945046  0.0888990484  0.1649976041  0.1408301905 \n           86            87            88            89            90 \n 0.1068063372  0.0345802700  0.1417181195  0.0948681447  0.1178417344 \n           91            92            93            94            95 \n 0.1354579948  0.0999344456  0.1068063372  0.1417181195  0.0900928722 \n           96            97            98            99           100 \n 0.1476872157  0.0948681447  0.1306827223  0.0948681447  0.0411462782 \n          101           102           103           104           105 \n 0.0650226633  0.1381366594  0.1596254083  0.1417181195  0.1175507060 \n          106           107           108           109           110 \n 0.1596254083  0.1068063372  0.1357490232  0.0769608559  0.2679719366 \n          111           112           113           114           115 \n 0.0608442914  0.1070973656  0.1187445298  0.2252854673  0.0859145003 \n          116           117           118           119           120 \n 0.1799203447  0.0614412033  0.1008372410  0.2312545636  0.0172698931 \n          121           122           123           124           125 \n 0.1297799269  0.1187445298  0.1476872157  0.0879962530  0.1485900111 \n          126           127           128           129           130 \n 0.1187445298  0.1485900111  0.1008372410  0.1238108307  0.1485900111 \n          131           132           133           134           135 \n 0.1008372410  0.0411462782  0.1366518186  0.1366518186  0.0650226633 \n          136           137           138           139           140 \n 0.1799203447  0.1787265209  0.1247136260  0.1447026676  0.1882770659 \n          141           142           143           144           145 \n 0.1864863415  0.1936492616  0.1303768388  0.1685790527  0.1094850019 \n          146           147           148           149           150 \n 0.0638288395  0.0644257514  0.1948430854  0.0877052246  0.1363459351 \n          151           152           153           154           155 \n 0.1187445298  0.1366518186  0.0948681447  0.0113007969  0.1068063372 \n          156           157           158           159           160 \n 0.1464933919  0.1798009601  0.1056125134  0.0715886715  0.1848746828 \n          161           162           163           164           165 \n 0.1205352541  0.0505774499  0.1319362271  0.0948681447  0.0745732196 \n          166           167           168           169           170 \n 0.1094850019  0.0918835965  0.1325853981  0.1524624882  0.1691759532 \n          171           172           173           174           175 \n 0.1253105266  0.1312796228  0.1596254083  0.1068063372  0.0621574998 \n          176           177           178           179           180 \n 0.1596254083  0.1835017934  0.1835017934  0.0829299521  0.0769608559 \n          181           182           183           184           185 \n 0.1127754335  0.1835017934  0.1655945046  0.1059035418  0.1655945046 \n          186           187           188           189           190 \n 0.0650226633  0.1638037802  0.0829299521  0.1366518186  0.0888990484 \n          191           192           193           194           195 \n 0.1605282037  0.1426209149  0.1536563120  0.1462023635  0.0411462782 \n          196           197           198           199           200 \n 0.3864510667  0.1775326971  0.1793234214  0.0948681447  0.0787515802 \n          201           202           203           204           205 \n 0.0941444263  0.0665075042  0.1261984669  0.1787265209  0.1774133125 \n          206           207           208           209           210 \n 0.1043590087  0.1836808590  0.1851731331  0.0709917596  0.0888990484 \n          211           212           213           214           215 \n 0.1799800257  0.1500748634  0.1238108307  0.1482244353  0.1518058840 \n          216           217           218           219           220 \n 0.1081792266  0.1313915857  0.0943309251  0.0508759001  0.0745732196 \n          221           222           223           224           225 \n 0.1118726381  0.1190355582  0.1443445136  0.1115816097  0.1273922907 \n          226           227           228           229           230 \n 0.0947487600  0.1895305706  0.0687831890  0.1610580014 -0.0222455235 \n          231           232           233           234           235 \n 0.1536563120  0.0301631402  0.0725437260  0.0763639440  0.0829299521 \n          236           237           238           239           240 \n 0.1187445298  0.0948681447  0.0948681447  0.0590535670  0.0590535670 \n          241           242           243           244           245 \n 0.1417181195  0.2014090822  0.1536563120  0.1247136260  0.1238108307 \n          246           247           248           249           250 \n 0.1596254083  0.1894708897  0.1655945046  0.1306827223  0.1247136260 \n          251           252           253           254           255 \n 0.1954399859  0.1417181195  0.1008372410  0.1008372410  0.1238108307 \n          256           257           258           259           260 \n 0.1715636008  0.1655945046  0.1247136260  0.1297799269  0.1068063372 \n          261           262           263           264           265 \n 0.1545591074  0.0232389894  0.0829299521  0.1306827223  0.1536563120 \n          266           267           268           269           270 \n 0.0769608559  0.0948681447  0.1187445298  0.1485900111  0.1417181195 \n          271           272           273           274           275 \n 0.1306827223  0.0700889642  0.1178417344  0.0530844708  0.0769608559 \n          276           277           278           279           280 \n 0.2431927561  0.1008372410  0.1724663962  0.1954399859  0.1127754335 \n          281           282           283           284           285 \n 0.1127754335 -0.0483901659  0.0999344456  0.1903736851  0.1187445298 \n          286           287           288           289           290 \n 0.0948681447  0.0650226633  0.1068063372  0.1476872157  0.0709917596 \n          291           292           293           294           295 \n 0.1068063372  0.1715636008  0.1297799269  0.1187445298  0.1158196626 \n          296           297           298           299           300 \n 0.0999344456 -0.0125755882  0.1664973000  0.1724663962  0.1247136260 \n          301           302           303           304           305 \n 0.1068063372  0.0652614326  0.0411462782  0.1536563120  0.1476872157 \n          306           307           308           309           310 \n 0.1745481490  0.1042396240  0.0697382548  0.1485900111  0.1204755732 \n          311           312           313           314           315 \n 0.0881827518  0.1223259784  0.1386738790  0.0681265961  0.1127754335 \n          316           317           318           319           320 \n 0.1492988745  0.0948681447  0.0615605879  0.1066272716  0.1066272716 \n          321           322           323           324           325 \n 0.1191026837  0.1287726133  0.0835268640  0.2730382375  0.1133723340 \n          326           327           328           329           330 \n 0.1829048929  0.1605282037  0.1605282037 -0.0066064920  0.1536563120 \n          331           332           333           334           335 \n 0.1357490232  0.1247136260  0.0888990484  0.0829299521 -0.0006373957 \n          336           337           338           339           340 \n 0.1187445298  0.1187445298  0.1426209149  0.1715636008  0.0709917596 \n          341           342           343           344           345 \n 0.1605282037  0.1187445298  0.0471153745  0.1844045888 -0.0015401911 \n          346           347           348           349           350 \n 0.0471153745  0.0351771819 -0.0125755882  0.0292080857 -0.0961429361 \n          351           352           353           354           355 \n 0.0761774451  0.1393379165  0.1297799269  0.1187445298  0.0709917596 \n          356           357           358           359           360 \n 0.0590535670  0.1178417344  0.0855563577  0.0910479267  0.0843028415 \n          361           362           363           364           365 \n 0.0229405391  0.0677087612  0.1405242957  0.1724663962  0.1168941017 \n          366           367           368           369           370 \n 0.1023220818  0.0943234919  0.0709917596  0.1994989732  0.1826661236 \n          371           372           373           374           375 \n 0.1072838759  0.1118726381  0.1127754335  0.1127754335  0.1417181195 \n          376           377           378           379           380 \n 0.1417181195  0.1715636008  0.1596254083  0.2133472748  0.1426209149 \n          381           382           383           384           385 \n 0.1238108307  0.1954399859  0.0530844708  0.1894708897  0.1605282037 \n          386           387           388           389           390 \n 0.1485900111  0.1963427813  0.1954399859  0.0590535670  0.1697728765 \n          391           392           393           394           395 \n 0.1835017934  0.1715636008  0.1476872157  0.0709917596  0.1655945046 \n          396           397           398           399           400 \n 0.1127754335  0.0948681447  0.1306827223  0.1127754335  0.1187445298 \n          401           402           403           404           405 \n 0.0530844708  0.2312545636  0.0113007969  0.1238108307  0.0530844708 \n          406           407           408           409           410 \n 0.0590535670  0.0769608559  0.1238108307  0.1118726381  0.1238108307 \n          411           412           413           414           415 \n 0.0292080857  0.4709212099  0.0530844708  0.0351771819  0.1247136260 \n          416           417           418           419           420 \n 0.1426209149  0.1238108307  0.1655945046  0.1417181195  0.1306827223 \n          421           422           423           424           425 \n 0.1247136260  0.1426209149  0.0939653493  0.1247136260  0.1596254083 \n          426           427           428           429           430 \n 0.1485900111  0.1485900111  0.1417181195  0.1068063372  0.1835017934 \n          431           432           433           434           435 \n 0.0465184626  0.1014341415  0.1626099564  0.1479334069  0.0491971272 \n          436           437           438           439           440 \n 0.0871083241  0.0441308264  0.0906897727  0.1094850019  0.0417431901 \n          441           442           443           444           445 \n 0.0853175884  0.1109847092  0.1602223088  0.0954650452  0.1312796228 \n          446           447           448           449           450 \n 0.2324483874  0.2043936303  0.0942712328 -0.0075689797  0.1074032377 \n          451           452           453           454           455 \n 0.1121785330  0.1056125134  0.1402332673  0.1835017934  0.1148571862 \n          456           457           458           459           460 \n 0.1360474735  0.2300607398  0.1358161488  0.0483091983  0.3169185078 \n          461           462           463           464           465 \n 0.2476770002  0.1152227621  0.2366267593  0.1163568822  0.1673852289 \n          466           467           468           469           470 \n 0.0614412033  0.0539723997  0.1217290779  0.1924554378  0.0542782946 \n          471           472           473           474           475 \n 0.0987406218  0.1840986939  0.1196324587  0.1321675632 -0.0627159975 \n          476           477           478           479           480 \n 0.1799203447  0.2993022702  0.1076942662  0.1015461043  0.0924805084 \n          481           482           483           484           485 \n 0.1644006808  0.1241167255  0.0832209805  0.1017251699  0.0136884331 \n          486           487           488           489           490 \n 0.1894708897  0.1536563120  0.1319884976  0.1485900111  0.0581507716 \n          491           492           493           494           495 \n 0.1426209149  0.1357490232  0.2202191664  0.1451802062  0.1849343865 \n          496           497           498           499           500 \n 0.0382214224  0.1316974578  0.1521640380  0.0462125791  0.1297799269 \n          501           502           503           504           505 \n 0.0103980015  0.0232389894 -0.0015401911  0.0103980015  0.0053317006 \n          506           507           508           509           510 \n 0.1238108307  0.1715636008  0.1655945046  0.0530844708  0.1187445298 \n          511           512           513           514           515 \n 0.1366518186  0.1775326971  0.1894708897  0.1655945046  0.1306827223 \n          516           517           518           519           520 \n 0.1476872157  0.1247136260  0.0351771819  0.0829299521  0.1655945046 \n          521           522           523           524           525 \n 0.1485900111  0.1596254083  0.1417181195  0.1485900111  0.0948681447 \n          526           527           528           529           530 \n 0.1426209149  0.1476872157  0.0709917596  0.1127754335  0.1426209149 \n          531           532           533           534           535 \n 0.0829299521  0.1596254083  0.0232389894  0.0769608559  0.1187445298 \n          536           537           538           539           540 \n 0.1536563120  0.0590535670  0.0709917596  0.1605282037  0.0650226633 \n          541           542           543           544           545 \n 0.1178417344  0.0650226633  0.1655945046  0.1127754335  0.1476872157 \n          546           547           548           549           550 \n 0.0948681447  0.0411462782  0.1247136260  0.1015535261  0.1604013972 \n          551           552           553           554           555 \n 0.1766447682  0.1142080266  0.1995586542  0.0346399624  0.0731928970 \n          556           557           558           559           560 \n 0.2259420715  0.1107459399  0.0928983434  0.1263849657  0.3452642933 \n          561           562           563           564           565 \n 0.1164762668  0.1273997239  0.0912866846  0.1798606410  0.1195801996 \n          566           567           568           569           570 \n 0.1097237598  0.0961216494  0.0418028825  0.1682806024  0.1584987100 \n          571           572           573           574           575 \n 0.0768414712  0.4367108464  0.1723992707  0.0807810738  0.1148646081 \n          576           577           578           579           580 \n 0.1112831594  0.2489827868  0.1272803393  0.1830316994  0.0129647147 \n          581           582           583           584           585 \n 0.0965394844  0.1257209396  0.1041799431  0.1525892947  0.0575612930 \n          586           587           588           589           590 \n 0.1173716404  0.1824944798  0.0327895457  0.0692010353  0.0650226633 \n          591           592           593           594           595 \n 0.0948681447  0.0999344456  0.2052964257  0.0948681447  0.1626099564 \n          596           597           598           599           600 \n 0.1357490232  0.1297799269  0.0641198679  0.0113007969 -0.0483901659 \n          601           602           603           604           605 \n 0.1187445298 -0.0125755882  0.1068063372  0.1068063372  0.0948681447 \n          606           607           608           609           610 \n 0.0172698931  0.0948681447  0.3864510667  0.2500646478  0.1297799269 \n          611           612           613           614           615 \n-0.0125755882  0.0590535670  0.1605282037  0.1844045888  0.0530844708 \n          616           617           618           619           620 \n 0.0829299521  0.0650226633  0.1187445298  0.0524875589  0.0327895457 \n          621           622           623           624           625 \n 0.5962722317  0.1187445298  0.0697979358  0.0948681447  0.0757670321 \n          626           627           628           629           630 \n 0.1921644094  0.0900928722  0.1279892026  0.1560439596  0.1199383536 \n          631           632           633           634           635 \n 0.0739763077  0.0966588690  0.1450085625  0.0924805084  0.1226170068 \n          636           637           638           639           640 \n 0.1175507060  0.0769608559  0.1127754335  0.1068063372  0.1187445298 \n          641           642           643           644           645 \n 0.0650226633  0.1724663962  0.0948681447  0.1476872157  0.1784354925 \n          646           647           648           649           650 \n 0.1596254083  0.1596254083  0.1366518186  0.1784354925  0.1306827223 \n          651           652           653           654           655 \n 0.0650226633  0.1306827223  0.0351771819  0.1775326971  0.1247136260 \n          656           657           658           659           660 \n 0.1247136260  0.0709917596  0.0999344456  0.0590535670  0.0650226633 \n          661           662           663           664           665 \n 0.1775326971  0.1127754335  0.0530844708  0.1306827223  0.1247136260 \n          666           667           668           669           670 \n 0.1118726381  0.0471153745  0.1187445298  0.0879962530  0.1008372410 \n          671           672           673           674           675 \n 0.0709917596  0.1476872157  0.1715636008  0.0948681447  0.1118726381 \n          676           677           678           679           680 \n 0.0709917596  0.1366518186  0.1306827223  0.1306827223  0.1775326971 \n          681           682           683           684           685 \n 0.0829299521  0.1366518186  0.1068063372  0.0172698931  0.1068063372 \n          686           687           688           689           690 \n 0.1476872157  0.0888990484  0.0769608559  0.0232389894  0.1724663962 \n          691           692           693           694           695 \n 0.0948681447  0.0232389894  0.0650226633  0.0283052903 -0.0066064920 \n          696           697           698           699           700 \n-0.0364519733  0.1306827223  0.0351771819  0.0351771819 -0.0185446845 \n          701           702           703           704           705 \n-0.0075092874  0.1655945046  0.1655945046  0.0509952848  0.0345802700 \n          706           707           708           709           710 \n 0.0906897727  0.1288919980  0.0229331059  0.0942712328  0.1169538054 \n          711           712           713           714           715 \n 0.1763388733  0.0787515802  0.1312796228  0.0897869773  0.1220201063 \n          716           717           718           719           720 \n 0.0500999226  0.0799454040  0.1384425429  0.1008372410  0.0817361283 \n          721           722           723           724           725 \n 0.1444116392  0.1342641710  0.1787265209  0.1139692573  0.1333613870 \n          726           727           728           729           730 \n 0.1426805958  0.0650226633  0.1805172453  0.0304019095  0.0083162487 \n          731           732           733           734           735 \n 0.1295411576  0.0578597432  0.0626350271  0.0957560736 -0.0036219438 \n          736           737           738           739           740 \n 0.0829299521  0.1420240144  0.0983824792  0.1396363667  0.1655945046 \n          741           742           743           744           745 \n 0.1605282037  0.1366518186  0.1008372410  0.1127754335  0.1008372410 \n          746           747           748           749           750 \n 0.1605282037  0.1238108307  0.0709917596  0.1894708897  0.1247136260 \n          751           752           753           754           755 \n 0.1008372410  0.0709917596  0.1187445298  0.2193163710  0.0829299521 \n          756           757           758           759           760 \n 0.1775326971  0.1238108307  0.1008372410  0.0292080857  0.0888990484 \n          761           762           763           764           765 \n 0.0641198679  0.1655945046  0.1127754335  0.1715636008  0.1536563120 \n          766           767           768           769           770 \n 0.1008372410  0.1357490232  0.1571183988  0.1545591074  0.1775326971 \n          771           772           773           774           775 \n 0.1596254083  0.1476872157  0.0471153745  0.1183789540  0.1417181195 \n          776           777           778           779           780 \n-0.0006373957  0.0530844708  0.1596254083  0.1775326971  0.1417181195 \n          781           782           783           784           785 \n 0.1596254083  0.1118726381  0.0530844708  0.0769608559  0.1125889347 \n          786           787           788           789           790 \n 0.1417181195  0.1357490232  0.1715636008  0.2969146226  0.1008372410 \n          791           792           793           794           795 \n 0.0462125791  0.1775326971  0.1068063372  0.1366518186  0.1724663962 \n          796           797           798           799           800 \n 0.2551309487  0.1605282037  0.1357490232  0.2252854673  0.0939653493 \n          801           802           803           804           805 \n 0.0351771819  0.0351771819  0.0581507716  0.3446673928  0.2560337441 \n          806           807           808           809           810 \n 0.1596254083  0.2611000449  0.1545591074  0.1485900111  0.1963427813 \n          811           812           813           814           815 \n 0.1784354925  0.0999344456  0.1835017934  0.1485900111  0.1059035418 \n          816           817           818           819           820 \n 0.1655945046  0.1835017934  0.1655945046  0.1017251699  0.4461420294 \n          821           822           823           824           825 \n 0.1068063372  0.0590535670  0.1178417344  0.1536563120  0.1715636008 \n          826           827           828           829           830 \n 0.0411462782  0.0888990484  0.1187445298  0.1485900111  0.1357490232 \n          831           832           833           834           835 \n 0.1664973000  0.1247136260  0.1366518186  0.1306827223  0.2193163710 \n          836           837           838           839           840 \n 0.1068063372  0.1536563120  0.1187445298  0.2261882627  0.1127754335 \n          841           842           843           844           845 \n 0.1008372410  0.1127754335  0.0888990484  0.0530844708  0.1247136260 \n          846           847           848           849           850 \n 0.0650226633  0.0650226633  0.1187445298  0.0292080857  0.1297799269 \n          851           852           853           854           855 \n 0.1068063372  0.0769608559  0.1894708897  0.0829299521  0.1059035418 \n          856           857           858           859           860 \n 0.0709917596  0.1366518186  0.0351771819  0.0715886715  0.1127754335 \n          861           862           863           864           865 \n 0.1199383536  0.1565214755  0.1211321774  0.0089131606  0.1271012737 \n          866           867           868           869           870 \n 0.1181476293  0.1202293706  0.1513283681  0.0560690189  0.1364056274 \n          871           872           873           874           875 \n 0.0984495933  0.1465530957  0.1306827223  0.0960619685  0.0924805084 \n          876           877           878           879           880 \n 0.0850117049  0.0841760350  0.0838178811  0.0515847635  0.2026029060 \n          881           882           883           884           885 \n 0.0985689780  0.0942712328  0.2133472748  0.1318765461  0.0853175884 \n          886           887           888           889           890 \n 0.1536563120  0.1429119433  0.1300858218  0.2918483217  0.1411212076 \n          891           892           893           894           895 \n 0.1470977371  0.1651169659  0.0206125838  0.1273922907  0.1638037802 \n          896           897           898           899           900 \n 0.1667883284  0.1333613870  0.1148571862  0.0873470820  0.1441057671 \n          901           902           903           904           905 \n 0.0399524544  0.1276384932  0.1444116392  0.1748540439  0.0697979358 \n          906           907           908           909           910 \n 0.0753491971  0.0626350271  0.1500748634  0.0545693116  0.1026279653 \n          911           912           913           914           915 \n 0.1026279653  0.1318765461  0.0017502405  0.0945622612  0.1279892026 \n          916           917           918           919           920 \n 0.1435088438  0.0769608559  0.1074032377  0.1978276336  0.0626350271 \n          921           922           923           924           925 \n 0.1055453992  0.1962831004  0.0626350271  0.0357740938  0.1448294741 \n          926           927           928           929           930 \n 0.0292080857  0.1321675632  0.1829048929  0.1276981742  0.1435088438 \n          931           932           933           934           935 \n 0.0942712328  0.0841237759  0.1664301744  0.0829299521  0.0286111738 \n          936           937           938           939           940 \n 0.1900677902  0.1008372410  0.0853175884 -0.0089941282  0.1715636008 \n          941           942           943           944           945 \n 0.0471153745  0.0292080857  0.0590535670  0.1775326971  0.0590535670 \n          946           947           948           949           950 \n 0.0113007969  0.0471153745  0.1178417344  0.0530844708  0.2193163710 \n          951           952           953           954           955 \n 0.1297799269  0.1068063372  0.1297799269  0.1178417344  0.1306827223 \n          956           957           958           959           960 \n 0.1068063372  0.1127754335  0.1476872157  0.1417181195  0.0820271567 \n          961           962           963           964           965 \n 0.1297799269  0.0590535670  0.1536563120  0.1178417344  0.1476872157 \n          966           967           968           969           970 \n 0.1426209149  0.0590535670  0.0232389894  0.1536563120  0.1008372410 \n          971           972           973           974           975 \n 0.1127754335  0.1655945046  0.1417181195  0.0948681447  0.0351771819 \n          976           977           978           979           980 \n 0.0709917596  0.0709917596  0.1357490232  0.1247136260  0.0948681447 \n          981           982           983           984           985 \n 0.1366518186  0.1366518186  0.0581507716  0.1775326971  0.1775326971 \n          986           987           988           989           990 \n 0.1059035418  0.1655945046  0.1118726381  0.0471153745  0.0530844708 \n          991           992           993           994           995 \n 0.1178417344  0.1366518186  0.1306827223 -0.0066064920  0.0411462782 \n          996           997           998           999          1000 \n 0.0232389894  0.0948681447  0.1127754335  0.2252854673  0.0292080857 \n         1001          1002          1003          1004          1005 \n 0.0888990484  0.1238108307  0.1357490232  0.1008372410  0.1008372410 \n         1006          1007          1008          1009          1010 \n 0.1605282037  0.1247136260  0.1536563120  0.2073781785  0.1655945046 \n         1011          1012          1013          1014          1015 \n 0.3694465733  0.1775326971  0.1247136260  0.1247136260  0.1545591074 \n         1016          1017          1018          1019          1020 \n 0.1008372410  0.1954399859  0.1306827223  0.1476872157  0.1835017934 \n         1021          1022          1023          1024          1025 \n 0.1118726381  0.3267601040  0.1605282037  0.1476872157  0.1775326971 \n         1026          1027          1028          1029          1030 \n 0.1247136260  0.0760580605  0.0760580605  0.0411462782  0.2909455263 \n         1031          1032          1033          1034          1035 \n 0.1118726381  0.1008372410  0.1844045888  0.1417181195  0.0769608559 \n         1036          1037          1038          1039          1040 \n 0.1844045888  0.2202191664  0.0888990484  0.1366518186  0.1187445298 \n         1041          1042          1043          1044          1045 \n 0.2014090822  0.1187445298  0.1596254083  0.1059035418  0.0351771819 \n         1046          1047          1048          1049          1050 \n-0.0125755882  0.0948681447  0.1178417344  0.1715636008  0.0709917596 \n         1051          1052          1053          1054          1055 \n 0.1238108307  0.0650226633  0.0999344456  0.0590535670  0.0829299521 \n         1056          1057          1058          1059          1060 \n 0.2023118776  0.1306827223  0.1426209149  0.1068063372  0.0948681447 \n         1061          1062          1063          1064          1065 \n 0.0590535670  0.0232389894  0.0769608559  0.1417181195  0.1835017934 \n         1066          1067          1068          1069          1070 \n 0.0641198679  0.1247136260  0.1485900111  0.1775326971  0.1008372410 \n         1071          1072          1073          1074          1075 \n 0.1590285078  0.1330703699  0.0888990484  0.1118726381  0.1402332673 \n         1076          1077          1078          1079          1080 \n 0.0590535670  0.1241167255  0.0888990484  0.1145661578  0.0686041234 \n         1081          1082          1083          1084          1085 \n 0.0888990484  0.1775326971  0.1247136260  0.1187445298  0.1127754335 \n         1086          1087          1088          1089          1090 \n 0.1655945046  0.1402332673  0.1655945046  0.1187445298  0.1118726381 \n         1091          1092          1093          1094          1095 \n 0.0888990484  0.1068063372  0.0769608559  0.1545591074  1.6945859460 \n         1096          1097          1098          1099          1100 \n 0.1476872157  0.1417181195  0.0829299521  0.0769608559  0.0709917596 \n         1101          1102          1103          1104          1105 \n 0.1247136260  0.1247136260  0.0641198679  0.1187445298  0.4461420294 \n         1106          1107          1108          1109          1110 \n 0.1238108307  0.1715636008  0.0292080857  0.0760580605  0.1238108307 \n         1111          1112          1113          1114          1115 \n 0.0760580605  0.2014090822  0.0351771819  0.0650226633  0.2073781785 \n         1116          1117          1118          1119          1120 \n 0.1247136260  0.1238108307  0.0053317006  0.1536563120 -0.0006373957 \n         1121          1122          1123          1124          1125 \n-0.0185446845  0.0888990484  0.0411462782  0.0700889642  0.0292080857 \n         1126          1127          1128          1129          1130 \n 0.0471153745  0.0351771819  0.0999344456  0.1536563120  0.1426209149 \n         1131          1132          1133          1134          1135 \n 0.0769608559  0.0769608559 -0.0125755882  0.1485900111  0.1715636008 \n         1136          1137          1138          1139          1140 \n 0.2073781785  0.0590535670  0.0879962530  0.0333864576  0.1715636008 \n         1141          1142          1143          1144          1145 \n 0.1020310648  0.1297799269  0.1253105266  0.1127754335  0.0521816754 \n         1146          1147          1148          1149          1150 \n 0.0601280062  0.0345802700  0.1115816097  0.1187445298  0.1605282037 \n         1151          1152          1153          1154          1155 \n 0.0650226633  0.1187445298  0.0650226633  0.1357490232  0.1127754335 \n         1156          1157          1158          1159          1160 \n 0.1655945046  0.0590535670  0.1187445298  0.0948681447  0.0709917596 \n         1161          1162          1163          1164          1165 \n 0.1536563120  0.0999344456  0.1127754335  0.0223361940  0.1008372410 \n         1166          1167          1168          1169          1170 \n 0.0590535670  0.0700889642  0.1357490232  0.0700889642  0.1247136260 \n         1171          1172          1173          1174          1175 \n 0.1357490232  0.1775326971  0.0769608559  0.1127754335  0.1894708897 \n         1176          1177          1178          1179          1180 \n 0.1068063372  0.1008372410  0.0888990484  0.1954399859  0.0471153745 \n         1181          1182          1183          1184          1185 \n 0.0939653493  0.1306827223  0.1476872157  0.0948681447  0.1247136260 \n         1186          1187          1188          1189          1190 \n 0.1306827223  0.1357490232  0.1357490232  0.1127754335  0.0888990484 \n         1191          1192          1193          1194          1195 \n 0.0939653493  0.1127754335  0.1715636008  0.0829299521  0.1357490232 \n         1196          1197          1198          1199          1200 \n 0.0053317006  0.1247136260  0.0471153745  0.1596254083  0.1596254083 \n         1201          1202          1203          1204          1205 \n 0.1417181195  0.1008372410  0.1127754335  0.1835017934  0.1178417344 \n         1206          1207          1208          1209          1210 \n 0.1426209149  0.2312545636  0.1775326971  0.1835017934  0.0948681447 \n         1211          1212          1213          1214          1215 \n 0.1247136260  0.1963427813  0.0053317006  0.1008372410  0.1008372410 \n         1216          1217          1218          1219          1220 \n 0.1118726381  0.1417181195  0.1127754335  0.1008372410  0.0769608559 \n         1221          1222          1223          1224          1225 \n 0.1417181195  0.1715636008  0.1059035418  0.1596254083  0.0590535670 \n         1226          1227          1228          1229          1230 \n 0.0888990484  0.1596254083  0.2790073338  0.0888990484  0.0471153745 \n         1231          1232          1233          1234          1235 \n 0.1536563120  0.0829299521  0.1008372410  0.1715636008  0.0530844708 \n         1236          1237          1238          1239          1240 \n 0.0769608559  0.1417181195  0.1187445298  0.0939653493 -0.0364519733 \n         1241          1242          1243          1244          1245 \n 0.1068063372  0.2073781785  0.1835017934  0.1357490232  0.1715636008 \n         1246          1247          1248          1249          1250 \n 0.0888990484  0.1715636008  0.0471153745  0.1247136260  0.1596254083 \n         1251          1252          1253          1254          1255 \n 0.0769608559  0.0769608559 -0.0543592622  0.1187445298  0.0530844708 \n         1256          1257          1258          1259          1260 \n 0.0292080857 -0.0185446845  0.1536563120  0.1366518186  0.1297799269 \n         1261          1262          1263          1264          1265 \n 0.0769608559  0.0053317006  0.1247136260  0.1664973000  0.1247136260 \n         1266          1267          1268          1269          1270 \n 0.0530844708  0.1059035418  0.3983892592  0.0551662235  0.0113007969 \n         1271          1272          1273          1274          1275 \n 0.1127754335  0.1154540981  0.1178417344  0.1008372410  0.0581507716 \n         1276          1277          1278          1279          1280 \n 0.2372236599  0.0172698931  0.1894708897  0.2321573590  0.1835017934 \n         1281          1282          1283          1284          1285 \n 0.3169185078  0.1462023635  0.1966338097  0.0829299521  0.1115816097 \n         1286          1287          1288          1289          1290 \n 0.1447026676  0.1181476293  0.0829299521  0.0632319390  0.1476872157 \n         1291          1292          1293          1294          1295 \n 0.0859145003  0.1417181195  0.1247136260  0.0495030107  0.0972557923 \n         1296          1297          1298          1299          1300 \n 0.1127754335  0.1306827223  0.0939653493  0.0900928722  0.0948681447 \n         1301          1302          1303          1304          1305 \n 0.1655945046  0.0089131606  0.1357490232  0.0650226633  0.1297799269 \n         1306          1307          1308          1309          1310 \n 0.1485900111  0.0172698931  0.0232389894  0.1894708897  0.1417181195 \n         1311          1312          1313          1314          1315 \n 0.1127754335  0.1715636008  0.1127754335  0.1008372410 -0.0066064920 \n         1316          1317          1318          1319          1320 \n 0.1068063372  0.1118726381  0.1068063372  0.0829299521  0.1417181195 \n         1321          1322          1323          1324          1325 \n 0.6679013869  0.1178417344  0.0471153745  0.0939653493  0.1306827223 \n         1326          1327          1328          1329          1330 \n 0.0948681447  0.1596254083  0.0999344456  0.2133472748 -0.0245137808 \n         1331          1332          1333          1334          1335 \n 0.0760580605  0.0471153745  0.1187445298  0.1417181195  0.1545591074 \n         1336          1337          1338          1339          1340 \n 0.1715636008  0.1476872157  0.0650226633  0.1297799269  0.2372236599 \n         1341          1342          1343          1344          1345 \n 0.0172698931  0.0172698931  0.0888990484  0.1187445298  0.0292080857 \n         1346          1347          1348          1349          1350 \n 0.1008372410  0.1655945046  0.1715636008  0.1127754335  0.2014090822 \n         1351          1352          1353          1354          1355 \n 0.1068063372  0.0471153745  0.1775326971  0.1306827223  0.0113007969 \n         1356          1357          1358          1359          1360 \n 0.1068063372  0.1476872157  0.0530844708  0.1306827223  0.1127754335 \n         1361          1362          1363          1364          1365 \n 0.1775326971  0.1715636008  0.1306827223  0.1187445298  0.1068063372 \n         1366          1367          1368          1369          1370 \n 0.1187445298  0.0411462782  0.1297799269  0.0769608559  0.1366518186 \n         1371          1372          1373          1374          1375 \n 0.1187445298  0.1178417344  0.1306827223  0.0590535670  0.1664973000 \n         1376          1377          1378          1379          1380 \n 0.1068063372  0.1068063372  0.1306827223  0.1306827223  0.1596254083 \n         1381          1382          1383          1384          1385 \n-0.0006373957  0.0888990484  0.1187445298  0.1357490232  0.2193163710 \n         1386          1387          1388          1389          1390 \n 0.1835017934  0.0700889642  0.0471153745  0.1724663962  0.3148219114 \n         1391          1392          1393          1394          1395 \n 0.1118726381  0.2321573590  0.1417181195  0.0521816754  0.1844045888 \n         1396          1397          1398          1399          1400 \n-0.0125755882  0.1775326971  0.0888990484  0.2202191664  0.1059035418 \n         1401          1402          1403          1404          1405 \n 0.0232389894  0.1476872157  0.0700889642  0.2142500702  0.1476872157 \n         1406          1407          1408          1409          1410 \n 0.0641198679  0.0793484921  0.1256015550  0.1518655877  0.0238359013 \n         1411          1412          1413          1414          1415 \n 0.1082911781  0.1176626574  0.2133472748  0.0634110046  0.1954399859 \n         1416          1417          1418          1419          1420 \n 0.0736181651  0.1297799269  0.1118726381  0.1492988745  0.0872873897 \n         1421          1422          1423          1424          1425 \n 0.1223856821  0.2128100552  0.0223361940  0.1187445298  0.1485900111 \n         1426          1427          1428          1429          1430 \n 0.1476872157  0.1306827223  0.1417181195  0.1417181195  0.1685790527 \n         1431          1432          1433          1434          1435 \n 0.0709917596  0.1127754335  0.0417431901  0.0888990484 -0.0280952408 \n         1436          1437          1438          1439          1440 \n 0.0829299521  0.1220201063  0.1605282037  0.1127754335  0.1476872157 \n         1441          1442          1443          1444          1445 \n 0.1545591074  0.2491618524  0.1894708897  0.1775326971  0.1476872157 \n         1446          1447          1448          1449          1450 \n 0.0820271567  0.1306827223  0.1306827223  0.1306827223  0.1775326971 \n         1451          1452          1453          1454          1455 \n 0.1306827223  0.1306827223  0.1306827223  0.1306827223  0.1306827223 \n         1456          1457          1458          1459          1460 \n 0.0411462782  0.0590535670 -0.0066064920  0.0053317006  0.0709917596 \n         1461          1462          1463          1464          1465 \n 0.1297799269  0.1297799269  0.0521816754  0.1775326971  0.1247136260 \n         1466          1467          1468          1469          1470 \n 0.1596254083  0.2482739235  0.1715636008  0.1229229017  0.1945520570 \n         1471          1472          1473          1474          1475 \n 0.1420240144  0.1199383536  0.1545591074  0.1611251042  0.1187445298 \n         1476          1477          1478          1479          1480 \n 0.1715636008  0.1536563120  0.1297799269  0.1775326971  0.1306827223 \n         1481          1482          1483          1484          1485 \n 0.1417181195  0.0530844708  0.1118726381  0.1894708897  0.1187445298 \n         1486          1487          1488          1489          1490 \n 0.0948681447  0.1775326971  0.0650226633  0.0829299521  0.1476872157 \n         1491          1492          1493          1494          1495 \n 0.0709917596  0.0769608559  0.0948681447  0.0888990484  0.1835017934 \n         1496          1497          1498          1499          1500 \n 0.0951069140  0.1228631980  0.1485303302  0.0590535670  0.1835017934 \n         1501          1502          1503          1504          1505 \n 0.0140988405  0.0948681447  0.1187445298  0.1306827223  0.0709917596 \n         1506          1507          1508          1509          1510 \n 0.1306827223  0.1357490232  0.1596254083 -0.0006373957  0.1476872157 \n         1511          1512          1513          1514          1515 \n 0.1835017934  0.0590535670  0.1187445298  0.1068063372  0.1187445298 \n         1516          1517          1518          1519          1520 \n 0.1476872157  0.1187445298  0.1247136260  0.0351771819  0.1068063372 \n         1521          1522          1523          1524          1525 \n 0.1068063372  0.1426209149 -0.0364519733  0.1894708897  0.0709917596 \n         1526          1527          1528          1529          1530 \n 0.1187445298  0.1835017934  0.0820271567  0.1655945046  0.1068063372 \n         1531          1532          1533          1534          1535 \n 0.1417181195  0.1366518186  0.1357490232  0.1247136260  0.1545591074 \n         1536          1537          1538          1539          1540 \n 0.0650226633  0.1715636008  0.1417181195  0.1238108307  0.1247136260 \n         1541          1542          1543          1544          1545 \n 0.1655945046  0.1187445298  0.1008372410  0.1238108307  0.0590535670 \n         1546          1547          1548          1549          1550 \n 0.0769608559  0.0948681447  0.0292080857  0.1417181195  0.0769608559 \n         1551          1552          1553          1554          1555 \n 0.1655945046  0.1187445298  0.0820271567  0.0530844708  0.1596254083 \n         1556          1557          1558          1559          1560 \n 0.1059035418  0.1476872157  0.0888990484  0.1068063372  0.0829299521 \n         1561          1562          1563          1564          1565 \n 0.1008372410  0.1366518186  0.1655945046  0.0709917596  0.1059035418 \n         1566          1567          1568          1569          1570 \n 0.1596254083  0.0769608559  0.1068063372  0.0829299521  0.0999344456 \n         1571          1572          1573          1574          1575 \n 0.0709917596  0.1655945046  0.1596254083  0.0888990484  0.0590535670 \n         1576          1577          1578          1579          1580 \n 0.0650226633  0.0948681447  0.1357490232  0.1306827223  0.0650226633 \n         1581          1582          1583          1584          1585 \n 0.0650226633  0.0879962530  0.1008372410  0.1297799269  0.1247136260 \n         1586          1587          1588          1589          1590 \n 0.1596254083  0.0829299521  0.3625746816  0.0471153745  0.0530844708 \n         1591          1592          1593          1594          1595 \n 0.1118726381  0.1238108307  0.1068063372  0.1008372410  0.2491618524 \n         1596          1597          1598          1599          1600 \n 0.2014090822  0.0948681447  0.1664973000  0.1775326971  0.1536563120 \n         1601          1602          1603          1604          1605 \n 0.0829299521  0.1835017934  0.0709917596  0.1417181195  0.1485900111 \n         1606          1607          1608          1609          1610 \n 0.1844045888  0.1596254083  0.0948681447  0.0769608559  0.1417181195 \n         1611          1612          1613          1614          1615 \n 0.0999344456  0.1068063372  0.1655945046  0.1476872157  0.0999344456 \n         1616          1617          1618          1619          1620 \n 0.0999344456  0.0650226633  0.0999344456  0.0411462782  0.0641198679 \n         1621          1622          1623          1624          1625 \n 0.0530844708  0.1059035418  0.0113007969  0.1068063372  0.1655945046 \n         1626          1627          1628          1629          1630 \n 0.1059035418  0.1775326971  0.1426209149  0.1297799269  0.0351771819 \n         1631          1632          1633          1634          1635 \n 0.0172698931  0.1835017934  0.1118726381  0.1655945046  0.0650226633 \n         1636          1637          1638          1639          1640 \n 0.1476872157  0.1417181195  0.1426209149  0.1187445298  0.0888990484 \n         1641          1642          1643          1644          1645 \n-0.0364519733  0.1118726381  0.1118726381  0.1715636008  0.1366518186 \n         1646          1647          1648          1649          1650 \n 0.0948681447  0.1306827223  0.1247136260  0.1536563120  0.1366518186 \n         1651          1652          1653          1654          1655 \n 0.0888990484  0.1127754335  0.0829299521  0.1118726381  0.0530844708 \n         1656          1657          1658          1659          1660 \n 0.1247136260  0.1068063372  0.1187445298  0.1476872157  0.1306827223 \n         1661          1662          1663          1664          1665 \n 0.1357490232  0.1297799269  0.1178417344  0.0888990484  0.0888990484 \n         1666          1667          1668          1669          1670 \n 0.1775326971  0.1068063372  0.1118726381  0.1178417344  0.0939653493 \n         1671          1672          1673          1674          1675 \n 0.1178417344  0.1357490232  0.0760580605  0.1536563120  0.0700889642 \n         1676          1677          1678          1679          1680 \n 0.1417181195  0.1426209149  0.1178417344  0.0888990484  0.0888990484 \n         1681          1682          1683          1684          1685 \n 0.1596254083  0.1247136260  0.0948681447  0.1068063372  0.1068063372 \n         1686          1687          1688          1689          1690 \n 0.1008372410  0.0351771819  0.1715636008  0.1963427813  0.1655945046 \n         1691          1692          1693          1694          1695 \n 0.1187445298  0.1536563120  0.1476872157  0.1178417344  0.1476872157 \n         1696          1697          1698          1699          1700 \n 0.1476872157  0.1187445298  0.1127754335  0.1715636008  0.0948681447 \n         1701          1702          1703          1704          1705 \n 0.1238108307  0.0650226633  0.1366518186  0.2014090822  0.1357490232 \n         1706          1707          1708          1709          1710 \n 0.1596254083  0.0769608559  0.0769608559  0.0709917596  0.1008372410 \n         1711          1712          1713          1714          1715 \n 0.2073781785  0.0888990484  0.1059035418  0.1068063372  0.1963427813 \n         1716          1717          1718          1719          1720 \n 0.1297799269  0.1476872157  0.1357490232  0.2193163710  0.0411462782 \n         1721          1722          1723          1724          1725 \n-0.0304828771  0.1715636008  0.0939653493  0.0172698931  0.1059035418 \n         1726          1727          1728          1729          1730 \n 0.1008372410  0.0650226633  0.0709917596  0.1247136260  0.0709917596 \n         1731          1732          1733          1734          1735 \n 0.2014090822  0.1366518186  0.0590535670  0.1008372410  0.1187445298 \n         1736          1737          1738          1739          1740 \n 0.1476872157  0.1844045888  0.2909455263  0.1655945046  0.1596254083 \n         1741          1742          1743          1744          1745 \n 0.0700889642 -0.0304828771  0.1247136260  0.0462125791  0.1297799269 \n         1746          1747          1748          1749          1750 \n 0.1127754335  0.1417181195  0.1008372410 -0.0185446845  0.0888990484 \n         1751          1752          1753          1754          1755 \n 0.0650226633  0.2849764300  0.1357490232  0.1357490232  0.1536563120 \n         1756          1757          1758          1759          1760 \n 0.0709917596  0.1247136260  0.1357490232  0.1417181195  0.1476872157 \n         1761          1762          1763          1764          1765 \n 0.0530844708  0.1844045888  0.0590535670  0.0948681447  0.1835017934 \n         1766          1767          1768          1769          1770 \n 0.1417181195  0.0530844708 -0.0304828771  0.1118726381  0.1357490232 \n         1771          1772          1773          1774          1775 \n 0.1655945046  0.0471153745  0.1655945046  0.1366518186  0.1596254083 \n         1776          1777          1778          1779          1780 \n 0.1476872157  0.1536563120  0.2551309487  0.0829299521  0.0471153745 \n         1781          1782          1783          1784          1785 \n 0.1297799269  0.1596254083  0.0879962530  0.0879962530  0.1187445298 \n         1786          1787          1788          1789          1790 \n 0.0709917596  0.1476872157  0.0888990484 -0.0254165762  0.1127754335 \n         1791          1792          1793          1794          1795 \n 0.0820271567  0.0590535670  0.0769608559  0.1357490232  0.1417181195 \n         1796          1797          1798          1799          1800 \n 0.1775326971  0.1068063372  0.1068063372  0.1238108307  0.1357490232 \n         1801          1802          1803          1804          1805 \n 0.0351771819  0.1536563120  0.1357490232  0.0888990484  0.0411462782 \n         1806          1807          1808          1809          1810 \n 0.1844045888  0.2014090822  0.1366518186  0.2014090822  0.1835017934 \n         1811          1812          1813          1814          1815 \n 0.1476872157  0.1178417344  0.2142500702 -0.0543592622  0.2133472748 \n         1816          1817          1818          1819          1820 \n 0.1187445298  0.2014090822  0.2133472748  0.1784354925  0.1844045888 \n         1821          1822          1823          1824          1825 \n 0.1306827223  0.1903736851  0.0709917596  0.1247136260  0.1596254083 \n         1826          1827          1828          1829          1830 \n 0.1426209149  0.1655945046  0.1068063372  0.1724663962  0.1426209149 \n         1831          1832          1833          1834          1835 \n 0.1306827223  0.0113007969  0.1306827223  0.1715636008  0.0462125791 \n         1836          1837          1838          1839          1840 \n 0.1715636008  0.0232389894  0.1417181195  0.1417181195  0.1655945046 \n         1841          1842          1843          1844          1845 \n 0.1068063372  0.1536563120  0.1187445298  0.0172698931  0.1536563120 \n         1846          1847          1848          1849          1850 \n 0.0879962530 -0.0125755882  0.1127754335  0.1008372410  0.0581507716 \n         1851          1852          1853          1854          1855 \n 0.1059035418  0.1068063372  0.1059035418  0.1059035418  0.0351771819 \n         1856          1857          1858          1859          1860 \n 0.0769608559  0.1238108307  0.0709917596  0.0530844708  0.1357490232 \n         1861          1862          1863          1864          1865 \n 0.1655945046  0.0471153745  0.1068063372  0.1247136260  0.1068063372 \n         1866          1867          1868          1869          1870 \n 0.0709917596  0.1187445298  0.1187445298  0.1127754335  0.0471153745 \n         1871          1872          1873          1874          1875 \n 0.1306827223  0.1127754335  0.1426209149  0.1426209149  0.1187445298 \n         1876          1877          1878          1879          1880 \n 0.1306827223  0.1426209149  0.1664973000  0.1485900111  0.1247136260 \n         1881          1882          1883          1884          1885 \n 0.1008372410  0.1366518186  0.1417181195  0.1306827223  0.0760580605 \n         1886          1887          1888          1889          1890 \n 0.2133472748  0.0172698931  0.1366518186  0.1426209149  0.1664973000 \n         1891          1892          1893          1894          1895 \n 0.1536563120  0.1655945046  0.0650226633  0.1366518186  0.1187445298 \n         1896          1897          1898          1899          1900 \n 0.1715636008  0.1187445298  0.1605282037  0.1366518186  0.1008372410 \n         1901          1902          1903          1904          1905 \n 0.2073781785  0.1068063372  0.0700889642  0.1178417344  0.2014090822 \n         1906          1907          1908          1909          1910 \n 0.1835017934  0.3924201630  0.0650226633  0.0939653493  0.1247136260 \n         1911          1912          1913          1914          1915 \n 0.2073781785  0.1536563120  0.2969146226  0.1426209149  0.1545591074 \n         1916          1917          1918          1919          1920 \n 0.2014090822  0.1247136260  0.1366518186  0.1835017934  0.1178417344 \n         1921          1922          1923          1924          1925 \n 0.1536563120  0.0530844708  0.1127754335  0.1008372410  0.1247136260 \n         1926          1927          1928          1929          1930 \n 0.0590535670  0.0471153745  0.7983187096  0.7983187096  0.1366518186 \n         1931          1932          1933          1934          1935 \n 0.1545591074  0.1485900111  0.1247136260  0.1059035418  0.1247136260 \n         1936          1937          1938          1939          1940 \n 0.2073781785  0.1894708897  0.2312545636  0.1187445298  0.1536563120 \n         1941          1942          1943          1944          1945 \n 0.0769608559  0.1835017934  0.0888990484  0.1426209149  0.1366518186 \n         1946          1947          1948          1949          1950 \n 0.1485900111  0.1775326971  0.1008372410  0.1835017934  0.0769608559 \n         1951          1952          1953          1954          1955 \n 0.1187445298  0.1187445298  0.1844045888  0.1536563120  0.1187445298 \n         1956          1957          1958          1959          1960 \n 0.1536563120  0.1775326971  0.1306827223  0.1426209149  0.2312545636 \n         1961          1962          1963          1964          1965 \n 0.1127754335  0.1485900111  0.1775326971  0.1306827223  0.0769608559 \n         1966          1967          1968          1969          1970 \n 0.1187445298  0.1835017934  0.1954399859  0.3267601040  0.1059035418 \n         1971          1972          1973          1974          1975 \n 0.0709917596  0.1357490232  0.0411462782  0.1545591074  0.1238108307 \n         1976          1977          1978          1979          1980 \n 0.0948681447  0.1596254083  0.1605282037  0.1485900111  0.1247136260 \n         1981          1982          1983          1984          1985 \n 0.1366518186  0.2252854673  0.1306827223  0.1008372410  0.1068063372 \n         1986          1987          1988          1989          1990 \n 0.0650226633  0.1187445298  0.1247136260  0.0530844708  0.1127754335 \n         1991          1992          1993          1994          1995 \n 0.0590535670  0.1068063372  0.1014341415  0.1276981742  0.0905629662 \n         1996          1997          1998          1999          2000 \n 0.1846956172  0.1118800600  0.0596504789  0.1429119433  0.0333864576 \n         2001          2002          2003          2004          2005 \n-0.0209323208  0.0906897727  0.0972557923  0.1312796228  0.1127754335 \n         2006          2007          2008          2009          2010 \n 0.0098010896  0.0814302448  0.1342641710  0.1199383536  0.0811392278 \n         2011          2012          2013          2014          2015 \n 0.1420240144  0.0751701315  0.1163568822  0.1109847092  0.1327644751 \n         2016          2017          2018          2019          2020 \n 0.0805423159  0.1265043504  0.0915777131  0.1032248886  0.1363459351 \n         2021          2022          2023          2024          2025 \n-0.0107848582  0.0787515802  0.0659105922  0.1624905718  0.1074032377 \n         2026          2027          2028          2029          2030 \n 0.1253105266  0.1536563120  0.1469709306  0.1139692573  0.1387932637 \n         2031          2032          2033          2034          2035 \n 0.1125366642  0.1566408602  0.1303768388  0.1050156129  0.1363459351 \n         2036          2037          2038          2039          2040 \n 0.1761598077  0.0954650452  0.1176104097  0.0990465166  0.1211321774 \n         2041          2042          2043          2044          2045 \n 0.0918239042  0.2145410986  0.1644006808  0.0972557923  0.1799203447 \n         2046          2047          2048          2049          2050 \n 0.1817110691  0.1195801996  0.1044186896  0.1579540686  0.0820271567 \n         2051          2052          2053          2054          2055 \n 0.0931967936  0.2599062211  0.1350327266  0.1005387907  0.0996434171 \n         2056          2057          2058          2059          2060 \n 0.1655945046  0.0912866846  0.1354579948  0.1721605014  0.1097908854 \n         2061          2062          2063          2064          2065 \n 0.1715636008  0.0965991881  0.1229229017  0.0817361283  0.1223259784 \n         2066          2067          2068          2069          2070 \n 0.0763639440  0.1253105266  0.0948681447  0.0883021365  0.1151630811 \n         2071          2072          2073          2074          2075 \n 0.1360549181  0.1345551994  0.1632068570  0.0930774204  0.0721855834 \n         2076          2077          2078          2079          2080 \n 0.0805423159  0.1235198022  0.1187445298  0.1267953788  0.1271535214 \n         2081          2082          2083          2084          2085 \n 0.1193414303  0.0703948477  0.0787515802  0.1262655811  0.0694994855 \n         2086          2087          2088          2089          2090 \n 0.1265640541  0.0136884331  0.0952262987  0.1217290779  0.1247136260 \n         2091          2092          2093          2094          2095 \n 0.0942712328  0.0868098624  0.1512686644  0.1211321774  0.0841237759 \n         2096          2097          2098          2099          2100 \n 0.1612370670  0.1513954937 -0.0030250319  0.0703948477  0.1870832420 \n         2101          2102          2103          2104          2105 \n 0.1287129096  0.1074032377  0.0434071023  0.1223259784  0.1715636008 \n         2106          2107          2108          2109          2110 \n 0.1638037802  0.1166479106  0.1057916018  0.0823330402  0.0894959603 \n         2111          2112          2113          2114          2115 \n 0.1336672705  0.1649976041  0.0978526928  0.1109847092  0.1441057671 \n         2116          2117          2118          2119          2120 \n 0.0811392278  0.0083759410  0.1239376372  0.0971364076  0.0892571910 \n         2121          2122          2123          2124          2125 \n 0.1159987510  0.0894959603  0.1211321774  0.1524624882  0.1047171626 \n         2126          2127          2128          2129          2130 \n 0.0381617301  0.0972557923  0.1439266787  0.0999344456  0.1655945046 \n         2131          2132          2133          2134          2135 \n 0.1127754335  0.1127754335  0.1596254083  0.1031592190  0.0834074794 \n         2136          2137          2138          2139          2140 \n 0.1315706513  0.1102684240  0.1414270911  0.0049138657  0.1072838759 \n         2141          2142          2143          2144          2145 \n 0.1210127927  0.0188815519  0.0099279074  0.1187445298  0.1420165697 \n         2146          2147          2148          2149          2150 \n 0.1664973000  0.1250643354  0.1396363667  0.1247136260  0.1664973000 \n         2151          2152          2153          2154          2155 \n 0.1715636008  0.1261387746  0.1693027825  0.1596254083  0.1649976041 \n         2156          2157          2158          2159          2160 \n 0.1835017934  0.1068063372  0.1388006969  0.0888990484  0.1307349814 \n         2161          2162          2163          2164          2165 \n 0.1589762487  0.1136111033  0.1366518186  0.1123575986  0.1260865155 \n         2166          2167          2168          2169          2170 \n 0.1040008547  0.1357564450  0.1550963270  0.0733793958  0.1151630811 \n         2171          2172          2173          2174          2175 \n 0.1890604766  0.1484706265  0.0198962987  0.1811215904  0.1546710475 \n         2176          2177          2178          2179          2180 \n 0.1403452301  0.1347342764  0.1618339675  0.1857700563  0.1379650271 \n         2181          2182          2183          2184          2185 \n 0.0235374397  0.2368655059  0.1366443854 -0.0283936911  0.0942115404 \n         2186          2187          2188          2189          2190 \n 0.1137901689  0.0496223954  0.0939130902  0.0654330650  0.1554544810 \n         2191          2192          2193          2194          2195 \n 0.1063885023 -0.0317363875  0.1106191334  0.1176700906  0.1703697770 \n         2196          2197          2198          2199          2200 \n 0.1022026971  0.0800647887  0.0788112725  0.1290636304  0.1523431036 \n         2201          2202          2203          2204          2205 \n 0.1239376372  0.1900081092  0.1127754335  0.1838002436  0.1895902743 \n         2206          2207          2208          2209          2210 \n 0.1398154323  0.1205352541  0.1229229017  0.1512089835  0.0953382387 \n         2211          2212          2213          2214          2215 \n 0.0516518890  0.1369502689  0.0961813531  0.1427328549  0.2053486848 \n         2216          2217          2218          2219          2220 \n 0.1353908806  0.1051349976  0.1059035418  0.1177297943  0.1142677076 \n         2221          2222          2223          2224          2225 \n 0.1196921510  0.0952859796  0.1614235772  0.1421359544  0.1414793502 \n         2226          2227          2228          2229          2230 \n 0.1423821456  0.0955247489  0.1799203447  0.0641198679 -0.0254165762 \n         2231          2232          2233          2234          2235 \n 0.1575959147  0.1927613327  0.1469112269  0.0749313622  0.0923611238 \n         2236          2237          2238          2239          2240 \n 0.1540218878  0.1762791924  0.2133472748  0.1429716242  0.1266834387 \n         2241          2242          2243          2244          2245 \n 0.0781546797  0.0847803688  0.0871680050  0.0948681447  0.1019116801 \n         2246          2247          2248          2249          2250 \n 0.1060303483  0.0945100021  0.0779756027  0.1005387907  0.1324734467 \n         2251          2252          2253          2254          2255 \n 0.0555914917  0.1393304832  0.1262655811  0.1645797691  0.1008372410 \n         2256          2257          2258          2259          2260 \n 0.1775923781  0.1398080105  0.1337866551  0.1568199258  0.0809601508 \n         2261          2262          2263          2264          2265 \n 0.0684847387  0.1431581344  0.1629681104  0.1310931240  0.0342818198 \n         2266          2267          2268          2269          2270 \n 0.0608442914  0.0966588690  0.1161181356  0.0877052246  0.1154540981 \n         2271          2272          2273          2274          2275 \n 0.1220872319  0.1022101304  0.1223185566  0.0603667641  0.1613564517 \n         2276          2277          2278          2279          2280 \n 0.1148049271  0.1303768388  0.1263252848  0.1596254083  0.1596254083 \n         2281          2282          2283          2284          2285 \n 0.0053317006  0.0521816754  0.0829299521  0.1068063372  0.2491618524 \n         2286          2287          2288          2289          2290 \n 0.0760580605  0.1187445298  0.0172698931  0.0948681447  0.1127754335 \n         2291          2292          2293          2294          2295 \n 0.0760580605  0.0709917596  0.0829299521  0.1187445298  0.0650226633 \n         2296          2297          2298          2299          2300 \n 0.1306827223  0.1536563120  0.1118726381  0.1247136260  0.0848997534 \n         2301          2302          2303          2304          2305 \n 0.0820942823  0.1248330107  0.1217290779  0.1223259784  0.1476872157 \n         2306          2307          2308          2309          2310 \n 0.1894708897  0.0650226633  0.0769608559  0.0888990484  0.1775326971 \n         2311          2312          2313          2314          2315 \n 0.1715636008  0.1775326971  0.1536563120  0.2014090822  0.0948681447 \n         2316          2317          2318          2319          2320 \n 0.1342641710  0.1578346840  0.0495030107  0.0411462782  0.1068063372 \n         2321          2322          2323          2324          2325 \n 0.1127754335  0.1769357966  0.1835017934  0.2014090822  0.0590535670 \n         2326          2327          2328          2329          2330 \n 0.1605282037  0.1068063372  0.0232389894  0.1187445298  0.0948681447 \n         2331          2332          2333          2334          2335 \n 0.1596254083  0.1844045888  0.1366518186  0.1068063372  0.1306827223 \n         2336          2337          2338          2339          2340 \n 0.1703697770  0.0829299521  0.1372487191  0.0829299521  0.1954399859 \n         2341          2342          2343          2344          2345 \n 0.1068063372  0.1059035418  0.1127754335 -0.0424210696  0.0590535670 \n         2346          2347          2348          2349          2350 \n 0.1664973000  0.1596254083  0.1366518186  0.0888990484  0.1426209149 \n         2351          2352          2353          2354          2355 \n 0.1485900111  0.0471153745  0.1008372410  0.1247136260  0.1127754335 \n         2356          2357          2358          2359          2360 \n 0.1545591074  0.1008372410  0.1068063372  0.1187445298  0.1417181195 \n         2361          2362          2363          2364          2365 \n 0.1417181195  0.0650226633  0.1008372410  0.1306827223  0.1187445298 \n         2366          2367          2368          2369          2370 \n 0.0939653493  0.0948681447  0.1775326971  0.1306827223  0.0999344456 \n         2371          2372          2373          2374          2375 \n 0.1417181195  0.1357490232  0.1008372410  0.1596254083  0.1008372410 \n         2376          2377          2378          2379          2380 \n 0.0888990484  0.1297799269  0.0590535670  0.1417181195  0.1596254083 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Porcentaje predicho correctamente\nppc <-data.frame(denymod2$model$deny, denymod2$fitted.values) \nhead(ppc)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  denymod2.model.deny denymod2.fitted.values\n1                   0             0.03577409\n2                   0             0.10888809\n3                   0             0.12590745\n4                   0             0.09486814\n5                   0             0.11874453\n6                   0             0.04711537\n```\n\n\n:::\n\n```{.r .cell-code}\n# Cambiar de nombre\n\nnames(ppc) <- c(\"deny\", \"VA\")\nhead(ppc)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  deny         VA\n1    0 0.03577409\n2    0 0.10888809\n3    0 0.12590745\n4    0 0.09486814\n5    0 0.11874453\n6    0 0.04711537\n```\n\n\n:::\n\n```{.r .cell-code}\n# Creando la variable y virgulilla\n\nppc$y.c <- ifelse(ppc$VA>0.5,1,0)\n\nhead(ppc)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  deny         VA y.c\n1    0 0.03577409   0\n2    0 0.10888809   0\n3    0 0.12590745   0\n4    0 0.09486814   0\n5    0 0.11874453   0\n6    0 0.04711537   0\n```\n\n\n:::\n\n```{.r .cell-code}\ntail(ppc)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     deny         VA y.c\n2375    0 0.10083724   0\n2376    0 0.08889905   0\n2377    0 0.12977993   0\n2378    0 0.05905357   0\n2379    1 0.14171812   0\n2380    1 0.15962541   0\n```\n\n\n:::\n\n```{.r .cell-code}\n# Creando la variable PPC\n\nppc$ppc <- ifelse(ppc$deny==ppc$y.c,1,0)\nhead(ppc)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  deny         VA y.c ppc\n1    0 0.03577409   0   1\n2    0 0.10888809   0   1\n3    0 0.12590745   0   1\n4    0 0.09486814   0   1\n5    0 0.11874453   0   1\n6    0 0.04711537   0   1\n```\n\n\n:::\n\n```{.r .cell-code}\n# Calculando el PPC\n\nprop.table(table(ppc$ppc))*100\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n       0        1 \n11.84874 88.15126 \n```\n\n\n:::\n:::\n\n\n\n\n## Ejemplo 2: Determinantes del trabajo femenino [@wooldridge2009]\n\n-   $y$: **inlf**: (la fuerza de trabajo femenino), una variable binaria que indica, si una mujer casada participó en la fuerza de trabajo durante 1975: $infl=1$ la mujer informa haber trabajado fuera de la casa, por un salario ese año, cero otro caso\n\n-   $x_1=nwifeinc$ los ingreso del esposo en miles de dólares (-)\n\n-   $x_2=educ$ años de educación (+)\n\n-   $x_3=exper$ años de experiencia (+)\n\n-   $x_4=exper^2$ años de experiencia al cuadrado (-)\n\n-   $x_5=edad$ en años (-)\n\n-   $x_6=kidslt6$ hijo \\< 6 años (-)\n\n-   $x_7=kidsge6$ hijos entres 6 y 18 años (+)\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(\"mroz\", package = \"wooldridge\")\nstr(mroz)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n'data.frame':\t753 obs. of  22 variables:\n $ inlf    : int  1 1 1 1 1 1 1 1 1 1 ...\n $ hours   : int  1610 1656 1980 456 1568 2032 1440 1020 1458 1600 ...\n $ kidslt6 : int  1 0 1 0 1 0 0 0 0 0 ...\n $ kidsge6 : int  0 2 3 3 2 0 2 0 2 2 ...\n $ age     : int  32 30 35 34 31 54 37 54 48 39 ...\n $ educ    : int  12 12 12 12 14 12 16 12 12 12 ...\n $ wage    : num  3.35 1.39 4.55 1.1 4.59 ...\n $ repwage : num  2.65 2.65 4.04 3.25 3.6 ...\n $ hushrs  : int  2708 2310 3072 1920 2000 1040 2670 4120 1995 2100 ...\n $ husage  : int  34 30 40 53 32 57 37 53 52 43 ...\n $ huseduc : int  12 9 12 10 12 11 12 8 4 12 ...\n $ huswage : num  4.03 8.44 3.58 3.54 10 ...\n $ faminc  : num  16310 21800 21040 7300 27300 ...\n $ mtr     : num  0.721 0.661 0.692 0.781 0.622 ...\n $ motheduc: int  12 7 12 7 12 14 14 3 7 7 ...\n $ fatheduc: int  7 7 7 7 14 7 7 3 7 7 ...\n $ unem    : num  5 11 5 5 9.5 7.5 5 5 3 5 ...\n $ city    : int  0 1 0 0 1 1 0 0 0 0 ...\n $ exper   : int  14 5 15 6 7 33 11 35 24 21 ...\n $ nwifeinc: num  10.9 19.5 12 6.8 20.1 ...\n $ lwage   : num  1.2102 0.3285 1.5141 0.0921 1.5243 ...\n $ expersq : int  196 25 225 36 49 1089 121 1225 576 441 ...\n - attr(*, \"time.stamp\")= chr \"25 Jun 2011 23:03\"\n```\n\n\n:::\n\n```{.r .cell-code}\n# Correr el modelo y crear el objeto\nmroz.mpl <- lm(inlf~\n                 nwifeinc+\n                 educ+\n                 exper+\n                 I(exper^2)+\n                 age+\n                 kidslt6+\n                 kidsge6,\n               data = mroz)\n\n\n# usando stargazer\nlibrary(stargazer)\n\nstargazer(mroz.mpl,\n          type = \"text\", \n          digits = 5)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n===============================================\n                        Dependent variable:    \n                    ---------------------------\n                               inlf            \n-----------------------------------------------\nnwifeinc                    -0.00341**         \n                             (0.00145)         \n                                               \neduc                        0.03800***         \n                             (0.00738)         \n                                               \nexper                       0.03949***         \n                             (0.00567)         \n                                               \nI(exper2)                   -0.00060***        \n                             (0.00018)         \n                                               \nage                         -0.01609***        \n                             (0.00248)         \n                                               \nkidslt6                     -0.26181***        \n                             (0.03351)         \n                                               \nkidsge6                       0.01301          \n                             (0.01320)         \n                                               \nConstant                    0.58552***         \n                             (0.15418)         \n                                               \n-----------------------------------------------\nObservations                    753            \nR2                            0.26422          \nAdjusted R2                   0.25730          \nResidual Std. Error     0.42713 (df = 745)     \nF Statistic          38.21795*** (df = 7; 745) \n===============================================\nNote:               *p<0.1; **p<0.05; ***p<0.01\n```\n\n\n:::\n\n```{.r .cell-code}\n# Punto de inflexión\nabs((coefficients(mroz.mpl)[4])/(coefficients(mroz.mpl)[5]*2))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   exper \n33.11387 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Probar la multicolinealidad aproximada\nlibrary(car)\n\nmean(vif(mroz.mpl))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 3.417547\n```\n\n\n:::\n\n```{.r .cell-code}\n# Normalidad de los errores\n\nlibrary(tseries)\njarque.bera.test(mroz.mpl$residuals)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tJarque Bera Test\n\ndata:  mroz.mpl$residuals\nX-squared = 36.741, df = 2, p-value = 1.051e-08\n```\n\n\n:::\n:::\n\n\n\n\n$$\nH_0: u\\sim N(\\mu, \\sigma^2)\n$$\n\n### El porcentaje predicho correctamente\n\nEs una medida de bondad de ajuste\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nPPC.DTF <- data.frame(mroz$inlf, mroz.mpl$fitted.values)\nnames(PPC.DTF) <- c(\"infl\", \"VA.infl\")\nPPC.DTF$ajuste <- ifelse(mroz.mpl$fitted.values>=0.5,1,0)\nPPC.DTF$PPC <- ifelse(PPC.DTF$infl==PPC.DTF$ajuste,1,0)\nprop.table(table(PPC.DTF$PPC))*100\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n       0        1 \n26.56042 73.43958 \n```\n\n\n:::\n:::\n\n\n\n\n### Interpretaciones ceteris paribus\n\n-   Para interpretar las estimaciones, hay que recordar que una variación en la variable independiente modifica la probabilidad de que $inlf=1$. Por ejemplo, *educ* si las demás variables permanecen constantes, una año más de educación hace que la probabilidad de participación en la fuerza laboral aumente en 3.8%. Si consideramos de forma literal a esta ecuación, entonces 10 años más educación incrementarían la probabilidad de permanecer en la fuerza laboral en 38%.\n\n-   El coeficiente de **nwifeinc** significa que si $\\Delta nwifeinc=10$ (un incremento de \\$10,000), la probabilidad de que una mujer permanecer en la fuerza de trabajo disminuye en 3.4%. Como se puede ver, esta disminución es pequeña a pesar de aumentar el salario en 10,000 dólares.\n\n-   La experiencia ha sido introducida como una función cuadrática para que el efecto de la experiencia sea decreciente sobre la probabilidad de participar en la fuerza laboral. *Ceteris paribus*, la variación de la probabilidad se aproxima como $0.039-2(0.0006)exper=0.039-0.0012exper$. El punto en el que la experiencia transcurrida no tiene efecto sobre la probabilidad de participación en la fuerza laboral es: $0.039/0.0012=32.5$. Sólo 13 mujeres de las 753 en esta muestra tiene más de 32 años de experiencia.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmroz$dico.exper <- ifelse(mroz$exper>32.5,1,0)\ntable(mroz$dico.exper)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n  0   1 \n740  13 \n```\n\n\n:::\n:::\n\n\n\n\n-   A diferencia de la cantidad de hijos entre 6 y 18 años, la cantidad de hijos menores a 6 años tiene un impacto enorme sobre la probabilidad de participación en la fuerza de trabajo. A tal punto que, tener un hijo menor a seis años adicional, reduce la probabilidad de participación en la fuerza trabajo en 26.18%. En la muestra, menos de 20% de las mujeres tienen al menos un hijo pequeño.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmroz$dic.hijo <- ifelse(mroz$kidslt6>=1,1,0)\nprop.table(table(mroz$dic.hijo))*100\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n       0        1 \n80.47809 19.52191 \n```\n\n\n:::\n:::\n\n\n\n\n-   Respecto al PPC el modelo predice en 73.44% a la variable **infl**.\n\n### Límites del MPL\n\n-   Las dos desventajas más importantes son que las probabilidades ajustadas pueden ser menores que cero o mayores que uno.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(mroz.mpl$fitted.values)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n-0.3451  0.4016  0.5880  0.5684  0.7592  1.1272 \n```\n\n\n:::\n:::\n\n\n\n\nSe demuestra para este ejemplo que algunos valores ajustados son menores que cero y mayores que uno.\n\n-   y, el efecto parcial de cualquier variable explicativa (si aparece en la ecuación en su nivel) es constante\n\n## Ejemplo 3: Un modelo de probabilidad lineal para arrestos [@wooldridge2009]\n\nSea **arr86** una variable binaria igual a uno si un hombre fue arrestado en 1986 e igual a cero si no fue así. La población es un grupo de hombres de California nacidos en 1960 o en 1961, que habían sido detenidos al menos una vez antes de 1986. Un modelo de probabilidad lineal para describir **arr86** es:\n\n$$arr86=\\beta_0+\\beta_1pcnv+\\beta_2avgsen+\\beta_3tottime+\\beta_4ptime86+\\beta_5qemp86+u$$ donde:\n\n-   $pcnv=$ proporción de arrestos previos que condujeron a una condena (+)\n\n-   $avgsen=$ sentencia promedio cumplida en condenas previas (en meses) (-)\n\n-   $tottime=$ meses en prisión y desde los 18 años de edad anteriores a 1986 (-)\n\n-   $ptime86=$ meses en prisión en 1986 (+ -)\n\n-   $qemp86=$ cantidad de trimestres (0 a 4) que el hombre estuvo empleado legalmente en 1986.(-)\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(\"crime1\", package = \"wooldridge\")\nstr(crime1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n'data.frame':\t2725 obs. of  16 variables:\n $ narr86 : int  0 2 1 2 1 0 2 5 0 0 ...\n $ nfarr86: int  0 2 1 2 1 0 2 3 0 0 ...\n $ nparr86: int  0 0 0 1 0 0 1 5 0 0 ...\n $ pcnv   : num  0.38 0.44 0.33 0.25 0 ...\n $ avgsen : num  17.6 0 22.8 0 0 ...\n $ tottime: num  35.2 0 22.8 0 0 ...\n $ ptime86: int  12 0 0 5 0 0 0 0 9 0 ...\n $ qemp86 : num  0 1 0 2 2 4 0 0 0 3 ...\n $ inc86  : num  0 0.8 0 8.8 8.1 ...\n $ durat  : num  0 0 11 0 1 ...\n $ black  : int  0 0 1 0 0 0 1 0 1 0 ...\n $ hispan : int  0 1 0 1 0 0 0 0 0 1 ...\n $ born60 : int  1 0 1 1 0 1 1 1 1 1 ...\n $ pcnvsq : num  0.1444 0.1936 0.1089 0.0625 0 ...\n $ pt86sq : int  144 0 0 25 0 0 0 0 81 0 ...\n $ inc86sq: num  0 0.64 0 77.44 65.61 ...\n - attr(*, \"time.stamp\")= chr \"25 Jun 2011 23:03\"\n```\n\n\n:::\n\n```{.r .cell-code}\n# Creando la variable y\ntable(crime1$narr86)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n   0    1    2    3    4    5    6    7    9   10   12 \n1970  559  121   42   12   13    4    1    1    1    1 \n```\n\n\n:::\n\n```{.r .cell-code}\ncrime1$arr86 <- ifelse(crime1$narr86>0,1,0)\n\narr86.MPL <- lm(arr86~\n                  pcnv+\n                  avgsen+\n                  tottime+\n                  ptime86+\n                  qemp86,\n                data = crime1)\nstargazer(arr86.MPL, \n          digits = 5,\n          type = \"text\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n===============================================\n                        Dependent variable:    \n                    ---------------------------\n                               arr86           \n-----------------------------------------------\npcnv                        -0.16244***        \n                             (0.02124)         \n                                               \navgsen                        0.00611          \n                             (0.00645)         \n                                               \ntottime                      -0.00226          \n                             (0.00498)         \n                                               \nptime86                     -0.02197***        \n                             (0.00463)         \n                                               \nqemp86                      -0.04283***        \n                             (0.00540)         \n                                               \nConstant                    0.44062***         \n                             (0.01723)         \n                                               \n-----------------------------------------------\nObservations                   2,725           \nR2                            0.04735          \nAdjusted R2                   0.04560          \nResidual Std. Error     0.43731 (df = 2719)    \nF Statistic         27.02966*** (df = 5; 2719) \n===============================================\nNote:               *p<0.1; **p<0.05; ***p<0.01\n```\n\n\n:::\n:::\n\n\n\n\n### Porcentaje predicho correctamente\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nPPC.arr <- data.frame(crime1$arr86, arr86.MPL$fitted.values)\nnames(PPC.arr)<- c(\"arr86\", \"valores_ajustados\")\nPPC.arr$ajuste <- ifelse(PPC.arr$valores_ajustados>=0.5,1,0)\nPPC.arr$PPC <-ifelse(PPC.arr$arr86==PPC.arr$ajuste,1,0)\nprop.table(table(PPC.arr$PPC))*100\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n       0        1 \n27.70642 72.29358 \n```\n\n\n:::\n:::\n\n\n\n\n### Interpretaciones\n\n-   $\\widehat{\\beta}_0=0.44062$, es la probabilidad de ser arrestado que se predice a un hombre, que no ha sido condenado, que no ha estado en prisión despues de los 18 años, que no ha estado en prisión en 1986 y que ha estado desempleado todo el año\n\n-   $\\widehat{\\beta}_2; \\widehat{\\beta}_3$ que pertenecen a las variables *avgsen* y *tottime*, respectivamente. No son estadísticamente significativas (no tiene asteriscos). El signo de *avgsen* es contrauntuitivo, pues se esperaria que condenas más largas disminuyan la probabilidad de ser arrestado en 1986.Con respecto a *tottime*, según los datos haber tenido meses en prisión de los 18 y antes de 1986, disminuye la probabilidad de ser arrestado en 1986.\n\n-   **ptimes86** el aumento de probabilidad de ser condenado en 1986, disminuye la probabilidad de ser arrestado en promedio en 2.2%. Si un hombre esta en prisión no puede ser arrestado. Como **ptimes86** esta medida en mese, 6 meses más en prisión, reduce la probabilidad de ser detenido en $0.0022\\times6\\approx0.132$. En esta variable se puede observar otra vez, que el MPL no cierto en todos los rangos de variables independientes. Por ejemplo, si un hombre está en prisión durante 12 meses de 1986, no puede ser detenido en 1986. **Ceteris paribus**, cuando $ptimr86=12$ la probabilidad predicha es de $0.44-0.22\\times12=0.177$ que es distinta de cero\n\n-   **qemp86** tener un empleo reduce la probabilidad de detención de manera significativa. **Ceteris paribus** un hombre que ha sido empleado durante 4 trimestres, la probabilidad de ser detenido se reduce en $0.04283\\times\\approx0.172$\n\n## Incorporando regresores binarios al MPL\n\nEn los modelos de variable dependiente binaria, se puede incluir variables independientes binarias. Este coeficiente mide la diferencia que se predice para la probabilidad en la relación con el grupo base. Así, incluimos regresores binarios en el MPL para **arr86**\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\narr86.MPL.bi <- lm(arr86~\n                  pcnv+\n                  avgsen+\n                  tottime+\n                  ptime86+\n                  qemp86+\n                    black+\n                    hispan,\n                data = crime1)\n\nstargazer(arr86.MPL.bi, \n          digits = 5,\n          type = \"text\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n===============================================\n                        Dependent variable:    \n                    ---------------------------\n                               arr86           \n-----------------------------------------------\npcnv                        -0.15206***        \n                             (0.02107)         \n                                               \navgsen                        0.00462          \n                             (0.00639)         \n                                               \ntottime                      -0.00256          \n                             (0.00493)         \n                                               \nptime86                     -0.02370***        \n                             (0.00459)         \n                                               \nqemp86                      -0.03847***        \n                             (0.00540)         \n                                               \nblack                       0.16976***         \n                             (0.02367)         \n                                               \nhispan                      0.09619***         \n                             (0.02071)         \n                                               \nConstant                    0.38043***         \n                             (0.01873)         \n                                               \n-----------------------------------------------\nObservations                   2,725           \nR2                            0.06819          \nAdjusted R2                   0.06579          \nResidual Std. Error     0.43265 (df = 2717)    \nF Statistic         28.40542*** (df = 7; 2717) \n===============================================\nNote:               *p<0.1; **p<0.05; ***p<0.01\n```\n\n\n:::\n\n```{.r .cell-code}\n# Limites del MPL\n\nsummary(arr86.MPL.bi$fitted.values)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-0.05598  0.21483  0.26501  0.27706  0.36119  0.61273 \n```\n\n\n:::\n:::\n\n\n\n\n### Interpretaciones\n\n-   El coeficiente **black** significa que, **ceteris paribus** un hombre negro tiene una probabilidad del 17% mayor de ser detenido frente a un hombre blanco. Otra forma de expresar esto, es que la probabilidad de ser detenido es de 17 puntos porcentuales mayor para los negros que para los blancos.\n\nDe la misma manera que la versión del modelo de regresión lineal múltiple en el MPL se puede verificar el cumplimento de los supuestos.\n\n### Supuestos\n\n#### Homocedasticidad\n\n-   La **homocedasticidad** o varianza constante, su incumplimiento se conoce como **heterocedásticidad** o varianza no constante\n\n-   Su incumplimento tiene efecto sobre la eficiencia de los estimadores de MCO.\n\n-   Su incumplimiento, hace que las pruebas $t$ o $f$ se invaliden, pues el cálculo de la varianza supone homocedásticidad que no se cumple. Por lo tanto, la matriz de varianza covarianza esta mal calculada.\n\n-   Existen dos formas de la heterocedasticidad, conocida y desconocida. Es común la forma desconocida, por tal motivo calculamos **errores estándar heterocedástico robustos**\n\n**Hipótesis**\n\n$$H_0:\\sigma^2$$\n\n$$H_a:\\sigma^2_i$$\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Test de homcedasticidad\nlibrary(lmtest)\n\n# El test de Breusch-Pagan\nbptest(mroz.mpl)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tstudentized Breusch-Pagan test\n\ndata:  mroz.mpl\nBP = 24.224, df = 7, p-value = 0.00104\n```\n\n\n:::\n\n```{.r .cell-code}\n# Test de Goldfeld-Quandt\n\ngqtest(mroz.mpl)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tGoldfeld-Quandt test\n\ndata:  mroz.mpl\nGQ = 2.488e+27, df1 = 369, df2 = 368, p-value < 2.2e-16\nalternative hypothesis: variance increases from segment 1 to 2\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(sandwich)\nrob_es <- list(sqrt(diag(vcovHC(mroz.mpl, type = \"HC1\"))))\n\nlibrary(stargazer)\n\nstargazer(mroz.mpl, mroz.mpl,\n          se = rob_es,\n          digits = 5,\n          type = \"text\",\n          column.labels = c(\"Heterocedástico-Robusto\", \"Homocedástico\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n====================================================================\n                                        Dependent variable:         \n                               -------------------------------------\n                                               inlf                 \n                               Heterocedástico-Robusto Homocedástico\n                                         (1)                (2)     \n--------------------------------------------------------------------\nnwifeinc                             -0.00341**         -0.00341**  \n                                      (0.00152)          (0.00145)  \n                                                                    \neduc                                 0.03800***         0.03800***  \n                                      (0.00727)          (0.00738)  \n                                                                    \nexper                                0.03949***         0.03949***  \n                                      (0.00581)          (0.00567)  \n                                                                    \nI(exper2)                            -0.00060***        -0.00060*** \n                                      (0.00019)          (0.00018)  \n                                                                    \nage                                  -0.01609***        -0.01609*** \n                                      (0.00240)          (0.00248)  \n                                                                    \nkidslt6                              -0.26181***        -0.26181*** \n                                      (0.03178)          (0.03351)  \n                                                                    \nkidsge6                                0.01301            0.01301   \n                                      (0.01353)          (0.01320)  \n                                                                    \nConstant                             0.58552***         0.58552***  \n                                      (0.15226)          (0.15418)  \n                                                                    \n--------------------------------------------------------------------\nObservations                             753                753     \nR2                                     0.26422            0.26422   \nAdjusted R2                            0.25730            0.25730   \nResidual Std. Error (df = 745)         0.42713            0.42713   \nF Statistic (df = 7; 745)            38.21795***        38.21795*** \n====================================================================\nNote:                                    *p<0.1; **p<0.05; ***p<0.01\n```\n\n\n:::\n:::\n\n\n\n\n#### Multicolinealidad aproximada\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(regclass)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nCargando paquete requerido: bestglm\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nCargando paquete requerido: leaps\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nCargando paquete requerido: VGAM\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nCargando paquete requerido: stats4\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nCargando paquete requerido: splines\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAdjuntando el paquete: 'VGAM'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following object is masked from 'package:AER':\n\n    tobit\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following object is masked from 'package:lmtest':\n\n    lrtest\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following object is masked from 'package:car':\n\n    logit\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nCargando paquete requerido: rpart\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nCargando paquete requerido: randomForest\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nrandomForest 4.7-1.2\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nType rfNews() to see new features/changes/bug fixes.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nImportant regclass change from 1.3:\nAll functions that had a . in the name now have an _\nall.correlations -> all_correlations, cor.demo -> cor_demo, etc.\n```\n\n\n:::\n\n```{.r .cell-code}\nVIF(mroz.mpl)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  nwifeinc       educ      exper I(exper^2)        age    kidslt6    kidsge6 \n  1.170686   1.166001   8.636160   8.770985   1.658272   1.270358   1.250370 \n```\n\n\n:::\n\n```{.r .cell-code}\nmean(VIF(mroz.mpl))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 3.417547\n```\n\n\n:::\n:::\n\n\n\n\nEn promedio el factor de inflación de la varianza es menor que 10. Por lo tanto, no debe preocuparme la multicolinealidad aproximada\n\n**Normalidad de los errores**\n\nLa $H_0$: los errores siguen una distribución normal\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tseries)\njarque.bera.test(mroz.mpl$residuals)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tJarque Bera Test\n\ndata:  mroz.mpl$residuals\nX-squared = 36.741, df = 2, p-value = 1.051e-08\n```\n\n\n:::\n:::\n\n\n\n\n## Modelos Logit y Probit para la respuesta binaria\n\nEl MPL es un modelo simple, que tiene varias desventajas. Las dos más importantes, como vimos en los ejemplos anteriores, son que las probabilidades ajustadas pueden ser menores que cero o mayores que uno y el efecto parcial de cualquier variable explicativa es constante. Esta limitaciones del MPL se superan con **modelos de respuesta binaria** más sofisticados.\n\nEn un modelo de respuesta binaria, el interés principal yace en la **probabilidad de respuesta**\n\n$$P(y=1|\\mathbf{x})=P(y=1|x_1,x_2,...,x_k)[6]$$ Donde: $\\mathbf{x}$ denota el conjunto total de variable explicativas. Por ejemplo, $\\mathbf{x}$ podría contener varias características individuales como la educación, edad, estado civil, etc., que afecta, por ejemplo, al estado del empleo, incluye una variable de binaria para la participación en reciente programa de empleo\n\n### Especificación del modelo logit y probit\n\nEn el MPL, se suponía que la probabilidad de respuesta es lineal al conjunto de parámetros, $\\beta_j$. Para evitar las limitaciones del MPL, considere una clase de modelos de respuesta binaria de la forma:\n\n$$P(y=1|\\mathbf{x})=G(\\beta_0+\\beta_1X_1+\\beta_2X_2+...+\\beta_kX_k)=G(\\beta_0+\\mathbf{x\\beta})[7]$$ donde $G(.)$ es una función que asume valores estrictamente entre 0 y 1: $0<G(.)<1$ para todos los número reales $z$. Esto asegura que las probabilidades de respuesta estimada sean estrictamente entre cero y uno. Note que: $\\mathbf{x\\beta}=\\beta_1X_1+\\beta_2X_2+...+\\beta_kX_k$\n\nSe han sugerido varias funciones no lineales para la función $G(.)$ a fin de asegurar que las probabilidades estén entre cero y uno. Las dos funciones que estudiaremos en esta clase, se usan en la mayoría de aplicaciones (junto con el MPL). En el **Modelo Logit**, $G(.)$ es la función logística:\n\n$$G(z)=\\frac{exp(z)}{[1+exp(z)]}=\\frac{e^z}{[1+e^z]}=\\Lambda(z) [8]$$ Que está entre cero y uno para todos los números reales $z$. Esta es la función de distribución acumulada (fda) para una variable aleatoria logística estándar.\n\nEn el **Modelo Probit**, $G(.)$ es la función de distribución acumulada normal estándar, que viene dada de la siguiente forma:\n\n$$G(z)=\\Phi(z)\\equiv\\int_{-\\infty}^z \\phi(v)dv[9]$$\n\nDonde:\n\n-   $\\phi(z)$ es la función de densidad normal estándar\n\n$$\\phi(z)=(2\\pi)^{-1/2}exp(-z/2)=\\frac{e^{-z/2}}{\\sqrt{2\\pi}}[10]$$ Esta elección de $G(.)$ asegura que $0<P(y=1|\\mathbf{x})<1$, para todos los valores de los parámetros y las $x_j$\n\nLas funciones $G(.)$ de **Logit** y **Probit** son crecientes. Cada una aumenta con más rapidez en $z=0$, $G(z)\\rightarrow 0$ a medida que $z\\rightarrow -\\infty$ y, $G(z)\\rightarrow 1$ a medida que $z\\rightarrow \\infty$\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nz<-seq(-10,10,0.1)\ny<-exp(z)/(1+exp(z))\nplot(y~z, \n     main = \"exp(z)/[1+exp(z)]\", \n     type =\"l\", col=\"blue\")\n```\n\n::: {.cell-output-display}\n![](cap1_files/figure-pdf/funcion-logistica-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\nLos modelos Logit y Probit pueden derivarse a partir de un **modelo de variable latente** subyacente.\n\nSea $y^*$ una variable inobservable, o *latente* determianda por:\n\n$$y^*=\\beta_0+ \\mathbf{x\\beta}+e, y=1[y^*>0][11]$$\n\nAquí se introduce la notación $1[.]$ para definir un resultado binario. La función $1[.]$ es la **función indicador**, que asume valor de uno si el evento dentro de los corchetes es verdadero y cero si es falso, Entonces tenemos:\n\n$$y=1[y^*>0][12]$$ $$y=0[y^*\\leq0]$$\n\nSe supone que $e$ es independiente de $\\mathbf{x}$ y que $e$ tiene un distribución logística estándar o normal estándar. En cualquier caso, $e$ se distribuye simétricamente en torno a cero, lo que significa que $1-G(-Z)=G(z)$ para todos los números reales de $z$.\n\nLos economistas tienden a favorecer el supuesto de normalidad para $e$, lo cual es la razón por la que en Econometría el **modelo Probit** es más popular que el **logit**. Además, varios problemas de especificación, que se tratarán después, se analizan fácilmente mediante Probit debido a las propiedades de la distribución normal.\n\nDado estos supuestos podemos calcular la probabilidad de respuesta para $y$:\n\n$$P(y=1|\\mathbf{x})=P(y^*>0|\\mathbf{x})=P(e>-(\\beta_0+\\mathbf{x\\beta})|\\mathbf{x})=\\\\1-G[-(\\beta_0+\\mathbf{x\\beta})]=G(\\beta_0+\\mathbf{x\\beta})[13]$$ uno de los objetivos de los modelos de respuesta binaria, es explicar los efectos de las $x_j$ sobre la probabilidad de respuesta $P(y=1|\\mathbf{x})$.Cuidado, la formula de la variable latente tiende a dar la impresión de que lo que principalmente interesa son los efectos de cada $x_j$ sobre $y^*$. Hay que aclarar que en los modelos Logit y Probit la dirección de efectos de $x_j$ sobre $E(y|\\mathbf{x})=P(y=1|\\mathbf{x})=G(\\beta_0+\\mathbf{x\\beta})$\n\nAclarando que:\n\n$$E(y^*|\\mathbf{x})=\\beta_0+\\mathbf{x\\beta}-[14]$$ Como la variable latente pocas veces tiene una unidad de medición definida, las magnitudes de cada $\\beta_j$ no son, útiles por sí mismas, a diferencia de las magnitudes calculadas por el **MPL**. Entonces para la mayoría de los propósitos, se requiere estimar el efecto de $x_j$ sobre la probabilidad de éxito $P(y=1|\\mathbf{x})$, esto se complicado por la naturaleza no lineal de $G(.)$. Esto nos lleva a definir tres casos de efectos parciales:\n\n### Variables aproximadamente continuas:\n\nPara hallar el efecto parcial de las variables aproximadamente continuas sobre la probabilidad de respuesta, se recurre al cálculo. Si $x_j$ es una variable aproximadamente continua, su efecto parcial sobre $p(x)=P(y=1|\\mathbf{x})$ se obtiene de la siguiente derivada parcial:\n\n$$\\frac{\\partial p(\\mathbf{x})}{\\partial x_j}=g(\\beta_0+\\mathbf{x\\beta})\\beta_j[15]$$\n\nDonde:\n\n$$g(z)\\equiv\\frac{dG}{dz}(z) [16]$$ Debido a que $G$ es la fda de una variable aleatoria continua, $g$ es la función de densidad de probabilidad.\n\nEn los casos de logit y probit, $G(.)$ es una fda estrictamente creciente y, por lo tanto, $g(z)>0\\forall z$. Por lo tanto, el efecto parcial de $x_j$ sobre $p(\\mathbf{x})$ depende de $\\mathbf{x}$ a través de la cantidad positiva $g(\\beta_0+\\mathbf{x\\beta})$, lo que significa que el efecto parcial siempre tiene el mismo signo que $\\beta_j$\n\nLa ecuación de la derivada parcial muestra que los efectos *relativos* del cualquiera las variables explicativas continuas no depende de $\\mathbf{x}$, la razón de los efectos parciales de $x_j$ y $x_h$ es $\\frac{\\beta_j}{\\beta_h}$. El caso típico de que $g$ sea un densidad simétrica en torno a cero, con una única moda en cero, el mayor efecto ocurre cuando $\\beta_0+\\mathbf{x\\beta}=0$. Por ejemplo:\n\n#### En el caso de Probit\n\n$$g(z)=\\phi(z)=\\frac{e^{-z/2}}{\\sqrt{2\\pi}}$$ $$g(0)=\\frac{e^{-0/2}}{\\sqrt{2\\pi}}=\\frac{1}{\\sqrt{2\\pi}}\\approx 0.40$$\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n1/sqrt(2*pi)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.3989423\n```\n\n\n:::\n:::\n\n\n\n\n#### En el caso logit\n\n$$g(z)=\\frac{e^z}{[1+e^z]^2}$$ Evaluando cuando $z=0$\n\n$$g(0)=\\frac{e^0}{[1+e^0]^2}$$\n\n$$g(0)=\\frac{1}{[1+1]^2}=\\frac{1}{4}=0.25$$\n\n### Cuando la variable explicativa es binaria\n\nEntonces el efecto parcial de cambiar$x_1$ de cero a uno, manteniendo constante todas las demás variables, es así:\n\n$$G(\\beta_0+\\beta_1+\\beta_2x_2+...+\\beta_kx_k)-G(\\beta_0+\\beta_2x_2+...+\\beta_kx_k) [17]$$\n\nDe nuevo, esto depende de todos los valores de las otras $x_j$. Por ejemplo, si $y$ es un indicador de empleo y $x_1$ es una variable binaria que indica la participación en un programa de capacitación laboral, entonces es el cambio en la probabilidad de empleo debido a este programa de capacitación; esto depende de las demás características que afectan la posibilidad de obtener el empleo, como la educación y la experiencia. Observe que saber el signo del $\\beta_1$ es suficiente para determinar si el programa tuvo un efecto positivo o negativo. Pero para hallar la **magnitud** del efecto, se tiene que estimar la cantidad usando la anterior ecuación \\[17\\].\n\n### Cuando la variable explicativa es discreta\n\nPor ejemplo, el número de hijos. Si $x_k$ denota esta variable, el efecto sobre la probabilidad de que $x_k$ cambien de $c_k$ a $c_k+1$ es simplemente:\n\n$$G[\\beta_0+\\beta_1x_1+\\beta_2x_2+...+\\beta_k(c_k+1)]-G[\\beta_0+\\beta_1x_1+\\beta_2x_2+...+\\beta_kc_k]$$\n\n### Estimación de máxima verosimilitud de los modelos Logit y Probit\n\nPara los MPL se uso mínimos cuadrados ordinarios (MCO) o, si existe heterocedasticidad, mínimos cuadrados ponderados (MCP). Ahora bien, debido a naturaleza no lineal $E(y|\\mathbf{x})$, MCO y MCP no son aplicables, por esta razón se usa la **estimación de máxima verosimilitud (EMV)**. Para estimar los modelos de variables dependientes limitadas, los métodos de máxima verosimilitud son indispensables. Como la EMV está basada en la distribución de $y$ dada $\\mathbf{x}$, la heterocedasticidad en $Var(y|\\mathbf{x})$ automáticamente se toma en cuenta.\n\nSuponiendo que se tiene una muestra aleatoria $n$. Para obtener el estimador de máxima verosimilitud, condicional sobre las variables explicativas, es necesario la densidad de $y_i$ dada $\\mathbf{x_i}$. Esto se escribe como:\n\n$$f(y|\\mathbf{x_i;\\beta})=[G(\\mathbf{x_i\\beta})]^y[1-G(\\mathbf{x_i\\beta})]^{1-y},y=0,1 [17]$$\n\nPara simplificar, se adsorbe el intercepto en el vector $\\mathbf{x_i}$. La **función log-verosimilitud** para cada observación $i$ es una función de los parámetros y los datos ($\\mathbf{x_i;y_i}$)), aplicando el logaritmo a la anterior ecuación tenemos:\n\n$$\\mathcal{l_i(\\beta)}=y_ilog[G(\\mathbf{x_i\\beta})]+(1-y)log[1-G(\\mathbf{x_i\\beta})] [18]$$ Como $G(.)$ está estrictamente definida entre cero y uno para logit y probit, $\\mathcal{l_i(\\beta)}$ está bien definida para todos los valores $\\beta$\n\nLa log-verosimilitud para un tamaño de muestra $n$ se obtiene al sumar todas las observación de la ecuación anterior:\n\n$$\\mathcal{L_i(\\beta)=\\Sigma_{i=1}^n}\\mathcal{l_i(\\beta)}[19]$$ La EMV de $\\beta$, denotada como $\\widehat{\\beta}$, maximiza esta log-verosimilitud. Si $G(.)$ es la fda logit estándar, entonces $\\widehat{\\beta}$ será el estimador Logit; si $G(.)$ es la fda normal estándar, entonces $\\widehat{\\beta}$ será el estimador Probit.\n\n### Ejemplos de aplicación\n\nContinuaremos con los ejemplos usados en el MPL, como son: la Participación en la fuerza laboral de las mujeres casadas, un modelo de probabilidad para arrestos y, sumaremos el ejemplo de la denegación de una hipoteca [@stock2012]\n\n#### Logit para los datos HMDA\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(AER)\ndata(HMDA)\n\nHMDA |>\n  str()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n'data.frame':\t2380 obs. of  14 variables:\n $ deny     : Factor w/ 2 levels \"no\",\"yes\": 1 1 1 1 1 1 1 1 2 1 ...\n $ pirat    : num  0.221 0.265 0.372 0.32 0.36 ...\n $ hirat    : num  0.221 0.265 0.248 0.25 0.35 ...\n $ lvrat    : num  0.8 0.922 0.92 0.86 0.6 ...\n $ chist    : Factor w/ 6 levels \"1\",\"2\",\"3\",\"4\",..: 5 2 1 1 1 1 1 2 2 2 ...\n $ mhist    : Factor w/ 4 levels \"1\",\"2\",\"3\",\"4\": 2 2 2 2 1 1 2 2 2 1 ...\n $ phist    : Factor w/ 2 levels \"no\",\"yes\": 1 1 1 1 1 1 1 1 1 1 ...\n $ unemp    : num  3.9 3.2 3.2 4.3 3.2 ...\n $ selfemp  : Factor w/ 2 levels \"no\",\"yes\": 1 1 1 1 1 1 1 1 1 1 ...\n $ insurance: Factor w/ 2 levels \"no\",\"yes\": 1 1 1 1 1 1 1 1 2 1 ...\n $ condomin : Factor w/ 2 levels \"no\",\"yes\": 1 1 1 1 1 1 2 1 1 1 ...\n $ afam     : Factor w/ 2 levels \"no\",\"yes\": 1 1 1 1 1 1 1 1 1 1 ...\n $ single   : Factor w/ 2 levels \"no\",\"yes\": 1 2 1 1 1 1 2 1 1 2 ...\n $ hschool  : Factor w/ 2 levels \"no\",\"yes\": 2 2 2 2 2 2 2 2 2 2 ...\n```\n\n\n:::\n:::\n\n\n\n\n#### La participación den la fuerza laboral de las mujeres casadas\n\n##### Modelo Probit estimado con fda normal estándar\n\nPara estimar modelos de variable dependiente limitada se usa el comando *glm()*\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(\"mroz\", package = \"wooldridge\")\n\nMPL.mroz <-lm(inlf~\n                 nwifeinc+\n                 educ+\n                 exper+\n                 expersq+\n                 age+\n                 kidslt6+\n                 kidsge6,\n               data = mroz) \n\n\nmroz.probit <- glm(inlf~\n                 nwifeinc+\n                 educ+\n                 exper+\n                 expersq+\n                 age+\n                 kidslt6+\n                 kidsge6,\n               data = mroz,\n               family = binomial(link = \"probit\"))\n\nlibrary(stargazer)\nstargazer(MPL.mroz, mroz.probit, type = \"text\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n=====================================================\n                           Dependent variable:       \n                    ---------------------------------\n                                  inlf               \n                              OLS            probit  \n                              (1)              (2)   \n-----------------------------------------------------\nnwifeinc                   -0.003**         -0.012** \n                            (0.001)          (0.005) \n                                                     \neduc                       0.038***         0.131*** \n                            (0.007)          (0.025) \n                                                     \nexper                      0.039***         0.123*** \n                            (0.006)          (0.019) \n                                                     \nexpersq                    -0.001***        -0.002***\n                           (0.0002)          (0.001) \n                                                     \nage                        -0.016***        -0.053***\n                            (0.002)          (0.008) \n                                                     \nkidslt6                    -0.262***        -0.868***\n                            (0.034)          (0.118) \n                                                     \nkidsge6                      0.013            0.036  \n                            (0.013)          (0.044) \n                                                     \nConstant                   0.586***           0.270  \n                            (0.154)          (0.508) \n                                                     \n-----------------------------------------------------\nObservations                  753              753   \nR2                           0.264                   \nAdjusted R2                  0.257                   \nLog Likelihood                              -401.302 \nAkaike Inf. Crit.                            818.604 \nResidual Std. Error    0.427 (df = 745)              \nF Statistic         38.218*** (df = 7; 745)          \n=====================================================\nNote:                     *p<0.1; **p<0.05; ***p<0.01\n```\n\n\n:::\n:::\n\n\n\n\n###### ¿Cómo funciona la mecánica del modelo Probit?\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmroz.probit.simple <-glm(inlf~\n                           kidslt6,\n                         data = mroz,\n                         family = binomial(link = \"probit\"))\n\nmroz.probit.simple1 <-glm(inlf~\n                           nwifeinc,\n                         data = mroz,\n                         family = binomial(link = \"probit\"))\n\nstargazer::stargazer(mroz.probit.simple1, type = \"text\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n=============================================\n                      Dependent variable:    \n                  ---------------------------\n                             inlf            \n---------------------------------------------\nnwifeinc                   -0.013***         \n                            (0.004)          \n                                             \nConstant                   0.432***          \n                            (0.094)          \n                                             \n---------------------------------------------\nObservations                  753            \nLog Likelihood             -509.662          \nAkaike Inf. Crit.          1,023.324         \n=============================================\nNote:             *p<0.1; **p<0.05; ***p<0.01\n```\n\n\n:::\n\n```{.r .cell-code}\nmean(mroz$kidslt6)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.2377158\n```\n\n\n:::\n\n```{.r .cell-code}\n# EFectos en cambios puntuales\n\nprediccion<-predict(mroz.probit.simple,\n                    newdata=data.frame(\"kidslt6\"=c(1,2)),\n                    type = \"response\")\n\ndiff(prediccion)*100\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       2 \n-18.7173 \n```\n\n\n:::\n:::\n\n\n\n\n#### Presentación del modelo estimado\n\n**Modelo probit simple**\n\n$$\n\\widehat{P(infl=1|kidslt6)}=\\Phi \\left(0.299-0.539kidslt6\\right)\n$$\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npnorm(coef(mroz.probit.simple)[1]+coef(mroz.probit.simple)[2]*2)-pnorm(coef(mroz.probit.simple)[1]+coef(mroz.probit.simple)[2]*1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(Intercept) \n  -0.187173 \n```\n\n\n:::\n:::\n\n\n\n\n**Ecuación estimada de probit**\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncoef(mroz.probit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n (Intercept)     nwifeinc         educ        exper      expersq          age \n 0.270073573 -0.012023637  0.130903969  0.123347168 -0.001887067 -0.052852442 \n     kidslt6      kidsge6 \n-0.868324680  0.036005611 \n```\n\n\n:::\n:::\n\n\n\n\n$$\n\\begin{aligned}\nP(inlf&=1|nwifeinc,...,kidsge6)=\\\\\n&\\Phi(0.27-0.012nwifeinc+0.131educ+0.12exper\\\\\n&-0.0019exper^2-0.053age-0.87kidslt6+0.036kidsg6)\n\\end{aligned}\n$$\n\n¿Cuál es la probabilidad de salir de que María salga a trabajar, dado que tiene su esposo un ingreso mensual 300USD, tiene 4 años de educación, nunca ha trabajo, tiene 29 años y un niño de 3 años?\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncoef(mroz.probit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n (Intercept)     nwifeinc         educ        exper      expersq          age \n 0.270073573 -0.012023637  0.130903969  0.123347168 -0.001887067 -0.052852442 \n     kidslt6      kidsge6 \n-0.868324680  0.036005611 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Pregunta inicial\nprediccion<-predict(mroz.probit,\n                    newdata=data.frame(\"nwifeinc\"=(300*12)/1000,\n                                       \"educ\"=4,\n                                       \"exper\"=0,\n                                       \"expersq\"=0,\n                                       \"age\"=29,\n                                       \"kidslt6\"=1,\n                                       \"kidsge6\"=0),\n                    type = \"response\")\n\npredict(mroz.probit,\n                    newdata=data.frame(\"nwifeinc\"=(300*12)/1000,\n                                       \"educ\"=4,\n                                       \"exper\"=0,\n                                       \"expersq\"=0,\n                                       \"age\"=29+3,\n                                       \"kidslt6\"=1,\n                                       \"kidsge6\"=1),\n                    type = \"response\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         1 \n0.03809838 \n```\n\n\n:::\n\n```{.r .cell-code}\n#Diferencia\ncambio <- predict(mroz.probit,\n                    newdata=data.frame(\"nwifeinc\"=(300*12)/1000,\n                                       \"educ\"=4,\n                                       \"exper\"=0,\n                                       \"expersq\"=0,\n                                       \"age\"=c(29,29+3),\n                                       \"kidslt6\"=1,\n                                       \"kidsge6\"=c(0,1)),\n                    type = \"response\")\n\ndiff(cambio)*100\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        2 \n-1.130756 \n```\n\n\n:::\n\n```{.r .cell-code}\n# En el MPL\n\ncambio.MPL <- predict(MPL.mroz,\n                    newdata=data.frame(\"nwifeinc\"=(300*12)/1000,\n                                       \"educ\"=4,\n                                       \"exper\"=0,\n                                       \"expersq\"=0,\n                                       \"age\"=c(29,29+3),\n                                       \"kidslt6\"=1,\n                                       \"kidsge6\"=c(0,1)),\n                    type = \"response\")\n\ndiff(cambio.MPL)*100\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        2 \n-3.526018 \n```\n\n\n:::\n:::\n\n\n\n\n#### Modelo Logit estimado con FDA logística estándar\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmroz.logit <- glm(inlf~\n                 nwifeinc+\n                 educ+\n                 exper+\n                 expersq+\n                 age+\n                 kidslt6+\n                 kidsge6,\n               data = mroz,\n               family = binomial(link = \"logit\"))\n\nstargazer(mroz.logit, type = \"text\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n=============================================\n                      Dependent variable:    \n                  ---------------------------\n                             inlf            \n---------------------------------------------\nnwifeinc                   -0.021**          \n                            (0.008)          \n                                             \neduc                       0.221***          \n                            (0.043)          \n                                             \nexper                      0.206***          \n                            (0.032)          \n                                             \nexpersq                    -0.003***         \n                            (0.001)          \n                                             \nage                        -0.088***         \n                            (0.015)          \n                                             \nkidslt6                    -1.443***         \n                            (0.204)          \n                                             \nkidsge6                      0.060           \n                            (0.075)          \n                                             \nConstant                     0.425           \n                            (0.860)          \n                                             \n---------------------------------------------\nObservations                  753            \nLog Likelihood             -401.765          \nAkaike Inf. Crit.           819.530          \n=============================================\nNote:             *p<0.1; **p<0.05; ***p<0.01\n```\n\n\n:::\n:::\n\n\n\n\n###### ¿Cómo funciona la mecánica del modelo Logit?\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmroz.logit.simple <-glm(inlf~\n                           kidslt6,\n                         data = mroz,\n                         family = binomial(link = \"logit\"))\n\nstargazer(mroz.probit.simple, type = \"text\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n=============================================\n                      Dependent variable:    \n                  ---------------------------\n                             inlf            \n---------------------------------------------\nkidslt6                    -0.539***         \n                            (0.094)          \n                                             \nConstant                   0.299***          \n                            (0.051)          \n                                             \n---------------------------------------------\nObservations                  753            \nLog Likelihood             -497.367          \nAkaike Inf. Crit.           998.734          \n=============================================\nNote:             *p<0.1; **p<0.05; ***p<0.01\n```\n\n\n:::\n\n```{.r .cell-code}\n# EFectos en cambios puntuales\n\nprediccion<-predict(mroz.logit.simple,\n                    newdata=data.frame(\"kidslt6\"=c(1,2)),\n                    type = \"response\")\ndiff(prediccion)*100\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        2 \n-18.29326 \n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlambda <- function(z) 1/(1+exp(-z))\n\nlambda(coef(mroz.logit.simple)[1]+coef(mroz.logit.simple)[2]*2)*100-lambda(coef(mroz.logit.simple)[1]+coef(mroz.logit.simple)[2]*1)*100\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(Intercept) \n  -18.29326 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Verificando que las probabilidades ajustadas se encuentren entre 0 y 1\n\nsummary(mroz.probit$fitted.values)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n0.002475 0.370959 0.609546 0.570109 0.794345 0.979904 \n```\n\n\n:::\n\n```{.r .cell-code}\nsummary(mroz.logit$fitted.values)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n0.008672 0.366410 0.610925 0.568393 0.796721 0.968541 \n```\n\n\n:::\n:::\n\n\n\n\n**Presentación del modelo estimado**\n\n$$P(inlf=1|nwifeinc,...,kidsge6)=\\Lambda(0.425-0.021nwifeinc+...+0.060kidsge6)$$\n\n#### Comparación de los modelos MPL, Logit y Probit\n\nPara esta comparación se va usar los errores heterocedástico robustos.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(sandwich)\n\neher<-list(sqrt(diag(vcovHC(MPL.mroz, type = \"HC1\"))),\n           sqrt(diag(vcovHC(mroz.logit, type = \"HC1\"))),\n           sqrt(diag(vcovHC(mroz.probit, type = \"HC1\"))))\n\nstargazer(MPL.mroz, mroz.logit, mroz.probit,\n          se = eher,\n          digits = 3,\n          type = \"text\",\n          title = \"Tabla 2: Estimaciones MPL, logit y probit de la participación en la fuerza laboral\",\n          df=F,\n          header = F)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nTabla 2: Estimaciones MPL, logit y probit de la participación en la fuerza laboral\n=================================================\n                         Dependent variable:     \n                    -----------------------------\n                                inlf             \n                       OLS    logistic   probit  \n                       (1)       (2)       (3)   \n-------------------------------------------------\nnwifeinc            -0.003**  -0.021**  -0.012** \n                     (0.002)   (0.009)   (0.006) \n                                                 \neduc                0.038***  0.221***  0.131*** \n                     (0.007)   (0.045)   (0.026) \n                                                 \nexper               0.039***  0.206***  0.123*** \n                     (0.006)   (0.032)   (0.019) \n                                                 \nexpersq             -0.001*** -0.003*** -0.002***\n                    (0.0002)   (0.001)   (0.001) \n                                                 \nage                 -0.016*** -0.088*** -0.053***\n                     (0.002)   (0.015)   (0.008) \n                                                 \nkidslt6             -0.262*** -1.443*** -0.868***\n                     (0.032)   (0.204)   (0.117) \n                                                 \nkidsge6               0.013     0.060     0.036  \n                     (0.014)   (0.080)   (0.047) \n                                                 \nConstant            0.586***    0.425     0.270  \n                     (0.152)   (0.864)   (0.507) \n                                                 \n-------------------------------------------------\nObservations           753       753       753   \nR2                    0.264                      \nAdjusted R2           0.257                      \nLog Likelihood                -401.765  -401.302 \nAkaike Inf. Crit.              819.530   818.604 \nResidual Std. Error   0.427                      \nF Statistic         38.218***                    \n=================================================\nNote:                 *p<0.1; **p<0.05; ***p<0.01\n```\n\n\n:::\n:::\n\n\n\n\nComo podemos ver en la tabla 2 los signos y la significancia es la misma para todas las variables en los tres modelos. Por ejemplo, la variable *educ* y *exper* son estadísticamente significativas en los tres modelos y ambas tienen un signo positivo respecto a la probabilidad de la participación en la fuerza laboral de las mujeres. En un primer momento no es posible comparar las estimaciones logit y probit con las del MPL. Para hacerlas comparables se debe usar el **efecto parcial promedio (EPP)**. Wooldridge (2010, 585) siguiere factores escalares que se deben pre-multiplicar por los coeficientes de logit y probit para hacerlos comparables con el MPL. Para Probit es 0.301 y para logit es 0.179.\n\n**Usando factores escalares logit y probit para comparar con coeficientes MPL**\n\nEl ejemplo de la variable *educ*. Si multiplico el coeficiente de *educ* en logit por su factor se obtiene: $0.179(0.221)\\approx0.040$ y coeficiente probit *educ* es de alrededor de $0.301(0.131)\\approx0.039$.Como se puede observar, ambos coeficientes son muy cercanos a la estimación de MPL que es de $0.038$. También la variable discreta *kidslt6*, los coeficientes escalados logit y probit son similares al coeficiente del MPL de $-0.262$. Estos son $0.179(-1.443)\\approx-0.258$ (logit) y, $0.301(-0.868)\\approx-0.261$ (probit)\n\nLa mayor diferencia entre el modelo MPL y los modelos logit y probit es que el MPL supone efectos *constantes* para *educ*, *exper*, *kidslt6*, etc., mientras que los modelos logit y probit implican magnitudes decrecientes de los efectos parciales\n\n#### Curva decreciente\n\nEn esta sección vamos a observar como los modelos no lineales logit y probit muestran que no es lo mismo tener niño pequeño, dos o tres, etc, para reducir la probabilidad de salir a trabajar\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmpl.simple <- lm(inlf~\n                   kidslt6,\n                 mroz)\nplot(x = mroz$kidslt6,\n     y= mroz$inlf,\n     main = \"Modelo probit para los determinates del trabajo femenino\",\n     xlab = \"Niños menore a seis años\",\n     ylab = \"Infl, si una mujer casada sale a trabajar por un salario\",\n     pch=20,\n     ylim = c(-0.4, 1.4),\n     xlim = c(-0.2,8))\ngrid()\n# Añadir las lineas horizontales y el texto\nabline(h=1, lty=2, col=\"darkred\")\nabline(h=0, lty=2, col=\"darkred\")\ntext(2.5, 0.9, cex = 0.8, \"Sale a trabajar\")\ntext(2.5, -0.1, cex = 0.8, \"No sale a trabajar\")\n\n# añadiendo la linea de regresión probit\nx <- seq(0,7,1)\ny <- predict(mroz.probit.simple, \n             list(kidslt6=x), \n             type = \"response\")\nlines(x,y,lwd=1.5, col=\"steelblue\")\n\n# añadiendo la linea de regresión logit\n\nt <- predict(mroz.logit.simple, \n             list(kidslt6=x), \n             type = \"response\")\nlines(x,t,lwd=1.5, col=\"pink\")\n\n# añadiendo la linea de regresión MPL\n\nm <- predict(mpl.simple, \n             list(kidslt6=x), \n             type = \"response\")\nlines(x,m,lwd=1.5, col=\"green\")\n```\n\n::: {.cell-output-display}\n![](cap1_files/figure-pdf/Curva decreciente-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n### Interpretaciones de las estimaciones Logit y Probit\n\nLas estimaciones de coeficientes, sus errores estándar y el valor de la función de log-verosimilitud se pueden obtener mediante todos los paquetes de software (R) que realicen logit y probit, y se deben reportar en cualquier aplicación. Los coeficientes dan los signos de los efectos parciales de cada $x_j$ sobre la probabilidad de respuesta y la significancia estadística de $x_j$ está determinada por si se puede rechazar $H_0:\\beta_j=0$ a un nivel de significancia ($\\alpha$).\n\nComo vimos anteriormente para el MPL se puede calcular el **porcentaje predicho correctamente**\n\nExisten varias medidas de bondad de ajuste como **pseudo R-cuadradas**. MacFadden (1974) sugiere la medida $1-\\frac{\\mathscr{L}_{nr}}{\\mathscr{L}_{o}}$, donde $\\mathscr{L}_{nr}$ es la función de log-verosimilitud para el modelo estimado y,$\\mathscr{L}_{o}$ es la función de probabilidad de log en el modelo con sólo un intercepto. ¿Por qué esta medida es lógica? Recordar que las log-verosimilitud son negativas y, por tanto $\\frac{\\mathscr{L}_{nr}}{\\mathscr{L}_{o}}=\\frac{|\\mathscr{L}_{nr}|}{|\\mathscr{L}_{o}|}$. Además. $|\\mathscr{L}_{nr}|\\leq|\\mathscr{L}_{o}|$. Si las covarianzas no tiene poder explicativo, entonces $\\frac{\\mathscr{L}_{nr}}{\\mathscr{L}_{o}}=1$, la **pseudo R-cuadrada** será igual a cero, como la R-cuadrada usual es cero en una regresión lineal cuando las covariadas no tienen poder explicativo.\n\nPor lo general, $|\\mathscr{L}_{nr}|<|\\mathscr{L}_{o}|$, en cuyo caso $1-\\frac{\\mathscr{L}_{nr}}{\\mathscr{L}_{o}}>0$. Supongamos que $\\mathscr{L}_{nr}\\rightarrow0$, la pseudo-Rcuadrada tiene a uno. Pero en los modelos logit y probit no pueden llegar a cero $\\mathscr{L}_{nr}$ ya que eso requeriría que las probabilidades estimadas cuando $y_i=1$ fueran iguales a la unidad y que las probabilidades estimadas cuando $y_i=0$ fueran todas iguales a cero\n\n### Cálculo de la speudo-$R^2$ de MacFadden\n\nSegun Stock y Watson (2011), las llamadas pseudo-$R^2$ se usan para medir la calidad del ajuste, estas medidas comparan el valor de la probabilidad máxima log-verosimulitud con todos los regresores, con la probabilidad de un modelo sin regresores (modelo nulo) **regresión en una constante**\n\nPor ejemplo, considere una regresión Probit. El **pseudo-**$R^2$ esta dado por:\n\n$$pseudo-R^2=1-\\frac{ln(f^{max}_{full})}{ln(f^{max}_{null})} [20]$$ Donde: $f^{max}_j\\in[0,1]$ denota la probabilidad máxima para el modelo $j$\n\nEl razonamiento detrás de esto, es que, la probabilidad maximizada aumenta a medida que se agregan regresores adicionales al modelo, de manera similar a la disminución en $SRC$ cuando se agregan regresores en un modelo de regresión lineal. Si el modelo completo tiene una probabilidad maximizada similar a la del modelo nulo, el modelo completo no mejora realmente sobre un modelo que usa solo la información en la variable dependiente, por lo que $pseudo-R^2\\approx0$. Si el modelo completo se ajusta muy bien a los datos, la probabilidad maximizada debe estar cerca de $1$, tal que $ln(f^{max}_{full})\\approx0$ y $pseudo-R^2\\approx1$\n\nEn Rstudio para los modelos estimados con $glm()$ podemos utilizar las entradas de desviación residual **(desviance)** y la desviación nula **(null.desviance)**. Estos han sido calculados de la siguiente forma:\n\n$$desviance=-2\\times[ln(f^{max}_{satured})-ln(f^{max}_{full})]\\\\null.desviance=-2\\times[ln(f^{max}_{satured})-ln(f^{max}_{null})]$$ Donde: $f^{max}_{satured}$ es la probabilidad maximizada para un modelo que asume que cada observación tiene su propio parámetro (hay $n+1$ parámetros a estimar que conducen a un ajuste perfecto). Para los modelos con una variable dependiente binaria, se tiene que:\n\n$$pseduo-R^2=1-\\frac{desviance}{null.desviance}=1-\\frac{ln(f^{max}_{full})}{ln(f^{max}_{null})} [21]$$ **Cálculo del** $pseudo-R^2$ **para los modelos Logit y Probit** del ejemplo, La participación en la fuerza laboral de las mujeres casadas\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Probit\n\npseudo.R2.P <- 1-(mroz.probit$deviance/mroz.probit$null.deviance)\n\npseudo.R2.P*100\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 22.05805\n```\n\n\n:::\n\n```{.r .cell-code}\n# Logit\n```\n:::\n\n\n\n\nSi usamos la interpretación usual del $R^2$ de la regresión lineal, diremos que según los **pseudo-R2** de logit y probit, aproximadamente la variación de la probabilidad de la participación en la fuerza laboral de las mujeres casadas esta explicada por las variables regresoras en aproximadamente un 22%.\n\nEn cualquier caso, la bondad de ajuste suele ser menos importante que intentar obtener estimaciones convincentes de los efectos **ceteris paribus** de las variables explicativas.\n\n### Efecto parcial promedio y el efecto parcial en el promedio\n\nParte importante de estos modelos es estimar los efectos de las $x_j$ sobre las probabilidades de respuesta, $P(y=1|\\mathbf{x})$. Si $x_j$ es aproximadamente continua teníamos:\n\n$$\\Delta \\hat{P}(y=1|\\mathbf{x})\\approx[g(\\hat{\\beta}_0+\\mathbf{x}\\hat{\\beta})\\hat{\\beta}_j]\\Delta x_j [22]$$\n\nEntonces, para pequeños cambios en $x_j$. Así que, para $\\Delta x_j=1$ el cambio en la probabilidad de éxito es aproximadamente $g(\\hat{\\beta}_0+\\mathbf{x}\\hat{\\beta})\\hat{\\beta}_j$. En comparación con el MPL, el costo de usar modelos probit y logit es que los efectos parciales en la ecuación anterior son más difíciles de resumir debido a que el factor de escala $g(\\hat{\\beta}_0+\\mathbf{x}\\hat{\\beta})$, depende de $\\mathbf{x}$. Una posibilidad es insertar valores interesante para las $x_j$ (medias, medianas, mínimos, máximos, cuartíles, etc.) y, ver como cambia $g(\\hat{\\beta}_0+\\mathbf{x}\\hat{\\beta})$. Pero, a pesar de ser un proceso atractivo es tedioso y puede dar como resultado demasiada información aun si el número de variables explicativas es moderado.\n\nComo resumen rápido para obtener magnitudes de efectos parciales, es útil tener un factor escalar único que se pueda multiplicar con cada $\\widehat{\\beta}_j$ (o al menos aquellos coeficiente de variables aproximadamente continuas). Un método que suele usarse en paquetes econométricos es reemplazar cada variable explicativas con su promedio muestral. En otras palabras, el factor de ajuste es:\n\n$$g(\\hat{\\beta}_0+\\bar{\\mathbf{x}}\\hat{\\beta})=g(\\hat{\\beta}_0+\\hat{\\beta}_1\\bar{x}_1+\\hat{\\beta}_2\\bar{x}_2+...+\\hat{\\beta}_k\\bar{x}_k) [23]$$ Donde: $g(.)$ es la densidad normal estándar $(\\phi)$ para el caso probit y, $g(z)=\\frac{exp(z)}{[1+exp(z)]^2}$ para logit. Cuando a la ecuación anterior se multiplica por $\\widehat{\\beta}_j$ obtenemos el efecto de $x_j$ para la persona promedio en la muestra. Por lo tanto, si multiplico el coeficiente $\\beta_j$ por la ecuación \\[23\\], se obtiene el **efecto parcial en el promedio (EPeP)**.\n\n#### Ejemplo con los determinantes del trabajo femenino\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(\"mroz\", package = \"wooldridge\")\n\nEpep.probit <- glm(inlf~nwifeinc,\n                  mroz, \n                  family = binomial(link = \"probit\")) \nstargazer::stargazer(Epep.probit, type = \"text\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n=============================================\n                      Dependent variable:    \n                  ---------------------------\n                             inlf            \n---------------------------------------------\nnwifeinc                   -0.013***         \n                            (0.004)          \n                                             \nConstant                   0.432***          \n                            (0.094)          \n                                             \n---------------------------------------------\nObservations                  753            \nLog Likelihood             -509.662          \nAkaike Inf. Crit.          1,023.324         \n=============================================\nNote:             *p<0.1; **p<0.05; ***p<0.01\n```\n\n\n:::\n:::\n\n\n\n\nUna vez que tengo el modelo, lo uso para ejemplificar el **EPeP**,\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndnorm(coef(Epep.probit)[1]+coef(Epep.probit)[2]*mean(mroz$nwifeinc))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(Intercept) \n  0.3929979 \n```\n\n\n:::\n\n```{.r .cell-code}\nphi <- function(z) (1/sqrt(2*pi))*exp(-z^2/2)\n\nphi(coef(Epep.probit)[1]+coef(Epep.probit)[2]*mean(mroz$nwifeinc))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(Intercept) \n  0.3929979 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Efecto de aumentar el salario en una unidad son 1000 USD\n\ndnorm(coef(Epep.probit)[1]+coef(Epep.probit)[2]*mean(mroz$nwifeinc))*coef(Epep.probit)[2]*100\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(Intercept) \n -0.5052942 \n```\n\n\n:::\n:::\n\n\n\n\nEl mismo ejemplo para **logit**\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nEpep.logit <- glm(inlf~nwifeinc,\n                  mroz, \n                  family = binomial(link = \"logit\")) \nstargazer::stargazer(Epep.logit, type = \"text\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n=============================================\n                      Dependent variable:    \n                  ---------------------------\n                             inlf            \n---------------------------------------------\nnwifeinc                   -0.021***         \n                            (0.007)          \n                                             \nConstant                   0.695***          \n                            (0.152)          \n                                             \n---------------------------------------------\nObservations                  753            \nLog Likelihood             -509.654          \nAkaike Inf. Crit.          1,023.309         \n=============================================\nNote:             *p<0.1; **p<0.05; ***p<0.01\n```\n\n\n:::\n\n```{.r .cell-code}\n# fda logística estándar\n\nlambda.minus <- function(z) exp(z)/(1+exp(z))^2\n\nlambda.minus(coef(Epep.logit)[1]+coef(Epep.logit)[2]*mean(mroz$nwifeinc))*coef(Epep.logit)[2]*100*100\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(Intercept) \n  -50.91089 \n```\n\n\n:::\n:::\n\n\n\n\nExisten dos problemas con el uso del **EPeP**. Primero, si algunas de las variables explicativas son discretas, sus promedios no representan a nadie en la muestra. Por ejemplo, si $x_1=mujeres$ y 47.5% de las muestra son mujeres ¿qué sentido tiene insertar $\\bar{x}_1=0.475$ para representar a la persona \"promedio\"?. Segundo, si una variable explicativa continua aparece como función no lineal, por ejemplo, como un log-natural o cuadrática, no es claro si se quiere promediar la función no lineal o insertar el promedio en la función no lineal. Por ejemplo, ¿Se debe usar $\\bar{log(ventas)}$ o $log(\\bar{ventas})$ para representar el tamaño promedio de la empresa?. Los paquetes econométrico se quedan en el primero, el paquete está programado para calcular los promedios de los regresores incluidos en la estimación probit o logit.\n\nUn método diferente para calcular un factor escalar elude la cuestión de qué valores a insertar para las variables explicativas. En lugar de ello, el segundo factor escalar resulta al promediar los efectos parciales individuales a través de la muestra, lo que genera en algunas veces llamado **efecto parcial promedio (EPP)**. Por ejemplo, para una variable aproximadamente continua el **EPP** es:\n\n$$n^{-1}\\sum_{i=1}^n[g(\\hat{\\beta}_0+\\mathbf{x}\\hat{\\beta})\\hat{\\beta}_j]=n^{-1}\\sum_{i=1}^n[g(\\hat{\\beta}_0+\\mathbf{x}\\hat{\\beta})]\\hat{\\beta}_j [24]$$\n\nEl término que se multiplica a $\\hat{\\beta}_j$ actúa como un factor escalar:\n\n$$\nn^{-1}\\sum_{i=1}^n[g(\\hat{\\beta}_0+\\mathbf{x}\\hat{\\beta})] [25]\n$$ Los factores escalares que sirven para obtener el *EPP* y *EPeP* que fueron detallados anteriormente de la aproximación del cálculo, ninguna es lógica para variables explicativas discretas. Es su lugar, se debe estimar directamente el cambio de probabilidad. Para un cambio $x_k$ de $c_k$ a $c_k+1$, es análogo al efecto parcial en el promedio:\n\n$$G[\\hat{\\beta}_0+\\hat{\\beta}_1\\bar{x}_1+...+\\hat{\\beta}_{k-1}\\bar{x}_{k-1}+\\hat{\\beta}_k(c_k+1)]-G[\\hat{\\beta}_0+\\hat{\\beta}_1\\bar{x}_1+...+\\hat{\\beta}_{k-1}\\bar{x}_{k-1}+\\hat{\\beta}_kc_k] [26]$$ El efecto parcial promedio es:\n\n$$n^{-1}\\sum_{i=1}^n(G[\\hat{\\beta}_0+\\hat{\\beta}_1x_{i1}+...+\\hat{\\beta}_{k-1}x_{ik-1}+\\hat{\\beta}_k(c_k+1)]-G[\\hat{\\beta}_0+\\hat{\\beta}_1x_{i1}+...+\\hat{\\beta}_{k-1}x_{ik-1}+\\hat{\\beta}_kc_k])[27]$$ La función anterior se puede interpretar de forma particular cuando $x_k$ es binaria. Para cada unidad $i$, se estima la diferencia predicha en la probabilidad de que $y_i=1$ cuando $x_k=1$ y $x_k=0$, de la siguiente forma:\n\n$$n^{-1}\\sum _{i=1}^nG[\\hat{\\beta}_0+\\hat{\\beta}_1x_{i1}+...+\\hat{\\beta}_{k-1}x_{ik-1}+\\hat{\\beta}_k]-G[\\hat{\\beta}_0+\\hat{\\beta}_1x_{i1}+...+\\hat{\\beta}_{k-1}x_{ik-1}] [28]$$ Para finalizar la aplicación de MPL, Logit y Probit. Es importante tener un tipo de efecto marginal que sea interpretable para los modelos no lineales (logit y probit), estos se obtienen de la siguiente manera usando el ejemplo de:\n\n##### Efecto parcial promedio ejemplo\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Para probit\n\nmean(dnorm(coef(Epep.probit)[1]+coef(Epep.probit)[2]*mroz$nwifeinc))*coef(Epep.probit)[2]*100\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  nwifeinc \n-0.4998448 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Para logit\n\nmean(lambda.minus(coef(Epep.logit)[1]+coef(Epep.logit)[2]*mroz$nwifeinc))*coef(Epep.logit)[2]*100\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  nwifeinc \n-0.5021655 \n```\n\n\n:::\n:::\n\n\n\n\n**\\[Participación en la fuerza laboral de las mujeres casadas\\]**\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(mfx)\n\n# Probando lo hecho a mano\n\nprobitmfx(inlf~\n            nwifeinc,\n          data = mroz)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCall:\nprobitmfx(formula = inlf ~ nwifeinc, data = mroz)\n\nMarginal Effects:\n              dF/dx  Std. Err.       z    P>|z|   \nnwifeinc -0.0050529  0.0015912 -3.1755 0.001496 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n\n```{.r .cell-code}\n# Efectos marginales Probit\nmarginales.probit<-probitmfx(inlf~\n            nwifeinc+\n            educ+\n            exper+\n            expersq+\n            age+\n            kidslt6+\n            kidsge6,\n          data = mroz)\n\nmarginales.probit$mfxest[1:7]*100\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1]  -0.46961881   5.11284287   4.81768957  -0.07370502  -2.06430891\n[6] -33.91499645   1.40630594\n```\n\n\n:::\n\n```{.r .cell-code}\n# Efectos marginales Logit\nmarginales.logit<-logitmfx(inlf~\n            nwifeinc+\n            educ+\n            exper+\n            expersq+\n            age+\n            kidslt6+\n            kidsge6,\n          data = mroz)\n\nmarginales.logit$mfxest[1:7]*100\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1]  -0.51900534   5.37773087   5.00569282  -0.07669166  -2.14030205\n[6] -35.09498193   1.46162143\n```\n\n\n:::\n\n```{.r .cell-code}\n# Comparación entre logit y probit\n\nvariables<-c(\"nwifeinc\", \"educ\", \"exper\", \"expersq\", \"age\", \"kidslt6\", \"kidsge6\")\n\ncomparacion <-data.frame(variables,marginales.logit$mfxest[1:7]*100, marginales.probit$mfxest[1:7]*100)\n\nnames(comparacion)<-c(\"Betas\", \"Logit\", \"Probit\")\n\ncomparacion\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     Betas        Logit       Probit\n1 nwifeinc  -0.51900534  -0.46961881\n2     educ   5.37773087   5.11284287\n3    exper   5.00569282   4.81768957\n4  expersq  -0.07669166  -0.07370502\n5      age  -2.14030205  -2.06430891\n6  kidslt6 -35.09498193 -33.91499645\n7  kidsge6   1.46162143   1.40630594\n```\n\n\n:::\n\n```{.r .cell-code}\n100000/12\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 8333.333\n```\n\n\n:::\n:::\n\n\n\n\nUna vez, establecidos los valores de los betas interpretables, podemos pasar a mirar la exactitud del estimaciones de los dos modelos no lineales.\n\n### Porcentaje predicho correctamente y la matriz de confusión\n\nEn lugar de solo calcular el PPC, se presentará la matriz de confusión que permite mostrar cuantas veces el modelo predijo correctamente los valores de $y$\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(vcd)\n\n#Probit\npredi.probit <- ifelse(mroz.probit$fitted.values>0.5,1,0)\n\nMCProbit <- table(mroz$inlf, predi.probit,\n                        dnn = c(\"observaciones\", \"estimaciones\"))\nPPC.probit <- data.frame(mroz$inlf, predi.probit, ifelse(mroz$inlf==predi.probit,1,0))\nnames(PPC.probit)=c(\"infl\", \"Valores ajustados\", \"PPC\")\n\nprop.table(table(PPC.probit$PPC))[2]*100\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       1 \n73.43958 \n```\n\n\n:::\n\n```{.r .cell-code}\nmosaic(MCProbit, \n       shade = T, \n       colorize = T,\n       gp = gpar(fill = matrix(c(\"blue\", \"yellow\", \"yellow\", \"blue\"),\n                               2,2)))\n```\n\n::: {.cell-output-display}\n![](cap1_files/figure-pdf/matriz_confusion Probit-1.pdf){fig-pos='H'}\n:::\n\n```{.r .cell-code}\nlibrary(gmodels)\n\nCrossTable(mroz$inlf, predi.probit,\n           digits = 2,\n           format = \"SPSS\",\n           prop.c = F,\n           prop.chisq = F,\n           prop.t = F,\n           dnn=c(\"Observado\", \"estimado\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n   Cell Contents\n|-------------------------|\n|                   Count |\n|             Row Percent |\n|-------------------------|\n\nTotal Observations in Table:  753 \n\n             | estimado \n   Observado |        0  |        1  | Row Total | \n-------------|-----------|-----------|-----------|\n           0 |      205  |      120  |      325  | \n             |    63.08% |    36.92% |    43.16% | \n-------------|-----------|-----------|-----------|\n           1 |       80  |      348  |      428  | \n             |    18.69% |    81.31% |    56.84% | \n-------------|-----------|-----------|-----------|\nColumn Total |      285  |      468  |      753  | \n-------------|-----------|-----------|-----------|\n\n \n```\n\n\n:::\n:::\n\n\n\n\n**1. Sensitividad**: % de positivos (1) que sob clasificados como positivos (1). para el modelo probit seria ($\\frac{347}{428}= 81.07\\%$)\n\n**2. Especificidad**: % negativos(0) que son clasificado como negativops(0). . en nuestro ejemplo\\_: ($\\frac{207}{325}= 63.98\\%$)\n\n**Falsos positivos:** % de negativos (o) clasificados como positivos (1). En nuestro ejemplo: ($\\frac{120}{325}= 36.92\\%$)\n\n**Falsos Negativos:** % de positivos clasificados (1) como negativos (0). en nuestro ejemplo. ($\\frac{80}{428}= 18.69\\%$)\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredi.logit <- ifelse(mroz.logit$fitted.values>=0.5,1,0)\n\nm.confu.logit<- table(mroz$inlf, predi.logit,\n                        dnn = c(\"observaciones\", \"estimaciones\"))\nPPC.logit <- data.frame(mroz$inlf, predi.logit, ifelse(mroz$inlf==predi.logit,1,0))\nnames(PPC.logit)=c(\"infl\", \"Valores ajustados\", \"PPC\")\n\nprop.table(table(PPC.logit$PPC))[2]*100\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       1 \n73.57238 \n```\n\n\n:::\n\n```{.r .cell-code}\nmosaic(m.confu.logit, shade = T, colorize = T,\n       gp = gpar(fill = matrix(c(\"green3\", \"red2\", \"red2\", \"green3\"),\n                               2,2)))\n```\n\n::: {.cell-output-display}\n![](cap1_files/figure-pdf/Logit PPC-1.pdf){fig-pos='H'}\n:::\n\n```{.r .cell-code}\nCrossTable(mroz$inlf, predi.logit,\n           digits = 2,\n           format = \"SPSS\",\n           prop.c = F,\n           prop.chisq = F,\n           prop.t = F,\n           dnn=c(\"Observado\", \"estimado\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n   Cell Contents\n|-------------------------|\n|                   Count |\n|             Row Percent |\n|-------------------------|\n\nTotal Observations in Table:  753 \n\n             | estimado \n   Observado |        0  |        1  | Row Total | \n-------------|-----------|-----------|-----------|\n           0 |      207  |      118  |      325  | \n             |    63.69% |    36.31% |    43.16% | \n-------------|-----------|-----------|-----------|\n           1 |       81  |      347  |      428  | \n             |    18.93% |    81.07% |    56.84% | \n-------------|-----------|-----------|-----------|\nColumn Total |      288  |      465  |      753  | \n-------------|-----------|-----------|-----------|\n\n \n```\n\n\n:::\n:::\n\n\n\n\nLos modelos logit y probit son capaces de clasificar correctamente el $73.5%$ de las observaciones cuando se emplean los datos de trabajo femenino.\n\n### Capacidad discriminante del modelo\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(\"Epi\")\n\nROC(form=inlf~nwifeinc+\n            educ+\n            exper+\n            expersq+\n            age+\n            kidslt6+\n            kidsge6,\n          data = mroz)\n```\n\n::: {.cell-output-display}\n![](cap1_files/figure-pdf/ROC-1.pdf){fig-pos='H'}\n:::\n\n::: {.cell-output-display}\n![](cap1_files/figure-pdf/ROC-2.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n**Sensibilidad**: la probabilidad de que el modelo prediga un resultado positivo (1) para una observación cuando en realidad el resultado es positivo (1)\n\n**Especificidad**: La probabilidad de que el modelo prediga un resultado negativo para una observación cuando en realidad el resultado es negativo.\n\n# Ejercicio 17.2 del libro de Wooldridge\n\nSea $grad$ una variable binaria para si un atleta colegial en una universidad grande se graduará en cinco años. Sean $hsGPA$y $SAT$ el promedio de calificaciones de bachillerato y las puntuaciones del $SAT$ de admisión a la universidad, respectivamente. Sea $study$ el número de horas por semana que pasa un estudiante en un aula de estudio. Suponga que, usando los datos sobre 420 atletas colegiales se obtiene el siguiente modelo logit:\n\n$$\n\\widehat{P}(grad=1|hsGPA,SAT,study)=\\Lambda(-1.17+0.24hsGPA+0.00058SAT+0.073study) \n$$ $$\n\\Lambda =\\frac{exp(z)}{[1+exp(z)]}\n$$ Si mantiene $hsGPA = 3.0$ y el $SAT = 1200$, calcule la diferencia estimada en la probabilidad de graduación para alguien que pasa 10 horas a la semana en el aula de estudio y alguien que pasa 5 horas por semana.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlambda <-(-1.17+0.24*3+0.00058*1200+0.073*10)\nlambda_5 <- (-1.17+0.24*3+0.00058*1200+0.073*5)\n\ndiferencia <-(exp(lambda)/(1+exp(lambda)))-(exp(lambda_5)/(1+exp(lambda_5)))\ndiferencia*100\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 7.814493\n```\n\n\n:::\n:::\n\n\n\n\n# Tareas\n\n1.  Tarea: realizar todos los cálculos para el modelo de los arrestos, igual como se hizo en clase para los dos modelos, es decir, con y sin variables binarias.\n",
    "supporting": [
      "cap1_files\\figure-pdf"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}